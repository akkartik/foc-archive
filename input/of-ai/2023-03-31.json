[
    {
        "user": "UE6EFEPTQ",
        "type": "message",
        "ts": "1680260866.732939",
        "client_msg_id": "c00263a4-9b21-4f8e-8d2b-4aac33ad30ad",
        "text": "Can't believe the internet is still arguing over whether ChatGPT is just \"glorified autocomplete\" :face_with_rolling_eyes:",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "8073c43d5d8d",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-12-18\/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
            "first_name": "Duncan",
            "real_name": "Duncan Cragg",
            "display_name": "Duncan Cragg",
            "team": "T5TCAFTA9",
            "name": "fp",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Z=9d8",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Can't believe the internet is still arguing over whether ChatGPT is just \"glorified autocomplete\" "
                            },
                            {
                                "type": "emoji",
                                "name": "face_with_rolling_eyes",
                                "unicode": "1f644"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UK3LH8CF5",
        "type": "message",
        "ts": "1680282970.147169",
        "client_msg_id": "f1169dcf-a2a6-47ce-befe-b2b47c10eaa8",
        "text": "It's a well written article. But there isn't much of an argument there. Can you define knowledge, understanding, intention, etc behavioristically? Well the article seems to assert you can, but why think that is the case?\n\nThe evidence we seem to be given in the article is two fold 1) Look at ChatGPT doing all these things, how can you deny it understands? 2) We can take the intentional stance towards the system and it works remarkably well.\n\nI'm going to assume the article doesn't believe its presenting an argument, because if so, its a rather lackluster one. Philosophers have already talked about systems like ChatGPT well before they existed. Searle's Chinese room argument asks the exact question being raised, can a computer program be behavioristically identical to a human while not understanding?\n\nVarious people fall on various sides of this argument. But I don't really think ChatGPT has really any bearing on it. Of course that doesn't stop philosophers like David Chalmers from embarrassing themselves by making silly statements.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "4377ee2417eb",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-12-25\/886144219253_4377ee2417eb9eaacd4b_72.jpg",
            "first_name": "Jimmy",
            "real_name": "Jimmy Miller",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "jimmyhmiller",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "c0Ep",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "It's a well written article. But there isn't much of an argument there. Can you define knowledge, understanding, intention, etc behavioristically? Well the article seems to assert you can, but why think that is the case?\n\nThe evidence we seem to be given in the article is two fold 1) Look at ChatGPT doing all these things, how can you deny it understands? 2) We can take the intentional stance towards the system and it works remarkably well.\n\nI'm going to assume the article doesn't believe its presenting an argument, because if so, its a rather lackluster one. Philosophers have already talked about systems like ChatGPT well before they existed. Searle's Chinese room argument asks the exact question being raised, can a computer program be behavioristically identical to a human while not understanding?\n\nVarious people fall on various sides of this argument. But I don't really think ChatGPT has really any bearing on it. Of course that doesn't stop philosophers like David Chalmers from embarrassing themselves by making silly statements."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "heavy_check_mark",
                "users": [
                    "U6KQ2S410",
                    "UA14TGLTC"
                ],
                "count": 2
            }
        ]
    },
    {
        "user": "UBKNXPBAB",
        "type": "message",
        "ts": "1680315015.475349",
        "client_msg_id": "49d6fb5b-5aff-43cf-beec-ba97015adda3",
        "text": "I found the article (or section 4 at least) quite useful. Not because it made an argument about what understanding etc. are in a philosophical sense (I sort of care about that stuff, but not really?), but because it helped me make sense of deflationary sleights of hand in the discourse.\n\nWhen Bender et al. call GPT a “stochastic parrot” or Chiang calls it a “blurry JPEG of the web”, they are _*not*_ making a Chinese-room argument about what understanding really is. They are making an argument about what GPT is _*capable of*_, based on its make-up, an argument which seems to be demonstrably flawed. The “deepity” concept helped me tidy up this dynamic. As far as I’m concerned, the rest of the article may be making a misstep by making it sound like this is about deep questions of philosophy.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "7b3bc9e878d6",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-28\/6866700980471_7b3bc9e878d663396caf_72.jpg",
            "first_name": "",
            "real_name": "Joshua Horowitz",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "joshuah",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "\/uJ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I found the article (or section 4 at least) quite useful. Not because it made an argument about what understanding etc. are in a philosophical sense (I sort of care about that stuff, but not really?), but because it helped me make sense of deflationary sleights of hand in the discourse.\n\nWhen Bender et al. call GPT a “stochastic parrot” or Chiang calls it a “blurry JPEG of the web”, they are "
                            },
                            {
                                "type": "text",
                                "text": "not",
                                "style": {
                                    "bold": true,
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " making a Chinese-room argument about what understanding really is. They are making an argument about what GPT is "
                            },
                            {
                                "type": "text",
                                "text": "capable of",
                                "style": {
                                    "bold": true,
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": ", based on its make-up, an argument which seems to be demonstrably flawed. The “deepity” concept helped me tidy up this dynamic. As far as I’m concerned, the rest of the article may be making a misstep by making it sound like this is about deep questions of philosophy."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UK3LH8CF5",
        "type": "message",
        "ts": "1680316282.013809",
        "edited": {
            "user": "UK3LH8CF5",
            "ts": "1680321133.000000"
        },
        "client_msg_id": "19b2238b-2c92-40cc-95be-7166b9c28a87",
        "text": "I need to spend more time reading the Stochastic Parrot paper, but from my reading they are not at questioning the output of GPT style models. They seem to think they are very capable of outputting fluent speech but lack things necessary for understanding.\n\n&gt; Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that. This can seem counter-intuitive given the increasingly fluent qualities of automatically generated text, but we have to account for the fact that our perception of natural language text, regardless of how it was generated, is mediated by our own linguistic competence and our predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do [89, 140]. The problem is, if one side of the communication does not have meaning, then the comprehension of the implicit meaning is an illusion arising from our singular human understanding of language (independent of the model).\nI'm not saying I necessarily agree with everything here. Just that this does seem to be the argument they are making. The deepity point was a bit off imo. From the article:\n\n&gt; The other meaning is the one that suggests that ChatGPT has no understanding of communicative intent, so when you ask it a question, it can only respond correctly in limited cases where it has seen the question, or else give awkward ill-fitting answers. But in this sense, ChatGPT is obviously not a stochastic parrot. You can ask it all sorts of subtle things, questions it has never seen before and which cannot be answered without understanding.\nThen there are some transcriptions demonstrating understanding of \"communicative intent\". But that's to misunderstand the point the paper was making, not to refute it. Of course GPT can talk about communicative intent. The point being made in the paper is that the ability to talk about communicative intent doesn't entail that it has communicative intent or pays attention to our communicative intent.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "4377ee2417eb",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-12-25\/886144219253_4377ee2417eb9eaacd4b_72.jpg",
            "first_name": "Jimmy",
            "real_name": "Jimmy Miller",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "jimmyhmiller",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Msyz6",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I need to spend more time reading the Stochastic Parrot paper, but from my reading they are not at questioning the output of GPT style models. They seem to think they are very capable of outputting fluent speech but lack things necessary for understanding.\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that. This can seem counter-intuitive given the increasingly fluent qualities of automatically generated text, but we have to account for the fact that our perception of natural language text, regardless of how it was generated, is mediated by our own linguistic competence and our predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do [89, 140]. The problem is, if one side of the communication does not have meaning, then the comprehension of the implicit meaning is an illusion arising from our singular human understanding of language (independent of the model)."
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nI'm not saying I necessarily agree with everything here. Just that this does seem to be the argument they are making. The deepity point was a bit off imo. From the article:\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "The other meaning is the one that suggests that ChatGPT has no understanding of communicative intent, so when you ask it a question, it can only respond correctly in limited cases where it has seen the question, or else give awkward ill-fitting answers. But in this sense, ChatGPT is obviously not a stochastic parrot. You can ask it all sorts of subtle things, questions it has never seen before and which cannot be answered without understanding."
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nThen there are some transcriptions demonstrating understanding of \"communicative intent\". But that's to misunderstand the point the paper was making, not to refute it. Of course GPT can talk about communicative intent. The point being made in the paper is that the ability to talk about communicative intent doesn't entail that it has communicative intent or pays attention to our communicative intent."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]