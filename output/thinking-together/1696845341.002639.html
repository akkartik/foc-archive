<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #thinking-together</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-09 02:55</span><br/>
Has anyone thought about or written about malleable software + LLM:s? I feel that it is an area that could be quite interesting, but I haven't got yet a clear understanding on what this will lead to.<br/><br/>I think that pretty soon most of end-user code will be generated with the help of LLM:s. Some thoughts/questions I've been thinking about:<br/>1. How to generate programming languages, libraries and abstractions that LLM:s can use well? Is that different from generating libraries etc for humans?<br/>    a. LLM:s are faster than humans in processing data, so API:s can be really wide and probably more complex than what would be practical for devs. <br/>    b. LLM:s can probably handle more condensed/optimized representation better, too. <br/>    c. And be able to infer system affordances directly from code.<br/>2. How to support creating good and maintainable code from LLM:s? And will that matter? Or will actual code become irrelevant?<br/>3. How to modularize apps so that they can be easily composed by LLM:s?<br/>    a. My hunch: making apps modular and composable would work really well with LLM:s already now and even better in the future. Doesn't matter if functional or OOP, as long as the LLM can understand the logic.<br/>4. What kinds of new apps and malleable software will LLM:s enable?<br/>5. Also: LLM:s could finally enable removing some boundaries between different programming tools, library ecosystems, etc, by enabling translations/bridges automatically.<br/>Any thoughts?
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696856872.883759"></a>
      <img src="https://avatars.slack-edge.com/2021-12-16/2861597891505_f3d63cdd315711ff17a4_72.jpg" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696856872.883759" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Paul Sonnentag</b>
<span style="margin:2em; color:#606060">2023-10-09 06:07</span><br/>
<a href="https://www.geoffreylitt.com/2023/03/25/llm-end-user-programming">https://www.geoffreylitt.com/2023/03/25/llm-end-user-programming</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696865948.141189"></a>
      <img src="https://avatars.slack-edge.com/2021-03-12/1859691333940_e169f54bbaf8b9b36b12_72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696865948.141189" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Konrad Hinsen</b>
<span style="margin:2em; color:#606060">2023-10-09 08:39</span><br/>
One big question I see is: will anyone actually make an LLM that will do this job well? Are there economic incentives?<br/><br/>Today's LLMs are not up to the task. Whatever they produce, code or prose, needs to be checked by a human if precision matters (which is almost always does in code).<br/><br/>Also, today's LLMs are not up to the taks of maintaining anything, because they themselves evolve too quickly. It's very much "move quickly and break things". If you write end-user code in a natural language and count on an LLM to turn it into precise instructions, at the very least you want those precise instructions to be reproducible over time.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696866838.238209"></a>
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696866838.238209" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-09 08:53</span><br/>
Thanks <span style="background-color:#ccf">@Paul Sonnentag</span>, this is pretty much exactly the kind of thing I was looking for and thinking about!
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696867174.255229"></a>
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696867174.255229" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-09 08:59</span><br/>
<span style="background-color:#ccf">@Konrad Hinsen</span>, good points! My hunch is that LLM:s will still improve a lot. Also model versioning, open-source models and fine-tuning will help at overcoming some of the other current bottle-necks. But we'll see. :man-shrugging:
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696871268.775599"></a>
      <img src="https://secure.gravatar.com/avatar/bc993d98fe7bf26c048ac0818a598d4d.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0001-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696871268.775599" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Mark Dewing</b>
<span style="margin:2em; color:#606060">2023-10-09 10:07</span><br/>
LLMs are good a producing a first draft, since they do require a lot of human oversight of the output.  I'm partial to modifying programs by describing the transformation of the code.  LLMs might be useful for writing the first draft of those transformations.  That helps with the reproducibility issue because the LLM is describing the changes to the code in more structured way, not the code itself.  Unfortunately, as far as I know, there is no widely recognized syntax or system for describing program transformations.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696872816.032239"></a>
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696872816.032239" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-09 10:33</span><br/>
Right. Instead or in addition to editing raw low-level imperative code, I think LLM:s could be useful in altering module-level code/structure, such as which objects/panels are visible where, how they are interconnected, etc. And then create minimal required glue to bind them together. I think this kind of use-case could be interesting to explore and perhaps even more interesting than just creating for loops, if-statements, etc.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696872877.922299"></a>
      <img src="https://avatars.slack-edge.com/2019-07-14/687915485201_6e649a383cf8f9e366e3_72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696872877.922299" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Kartik Agaram</b>
<span style="margin:2em; color:#606060">2023-10-09 10:34</span><br/>
At least in my filter bubble, I'm seeing a lot of ferment in the area of local-only models. I think they go a long way towards addressing concerns of incentives as well as LLM evolution.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696873043.779909"></a>
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696873043.779909" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-09 10:37</span><br/>
Interesting idea of having LLM:s describe transformations of code <span style="background-color:#ccf">@Mark Dewing</span>, btw. Using an LLM to create a transpiler from one language to another would be something like this. Perhaps also LLM creating glue code between API layers. Or are you thinking of something different. :thinking_face:
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696873468.339889"></a>
      <img src="https://avatars.slack-edge.com/2021-09-13/2508698086192_565c54a4fa91a0c8c75a_72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696873468.339889" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Tom Lieber</b>
<span style="margin:2em; color:#606060">2023-10-09 10:44</span><br/>
OpenAI had a first-class “edit” API where you’d send it (for example) code and a description of a change, and it would return the modified version. They deprecated it this summer because it was just as reliable to use the chat API. Even without a transformation language, I find it easy to work with because I can examine the diff just like if another human made the change.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696883025.497729"></a>
      <img src="https://avatars.slack-edge.com/2023-06-13/5441522160256_154a9d12968ca5a13cf5_72.jpg" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696883025.497729" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Greg Bylenok</b>
<span style="margin:2em; color:#606060">2023-10-09 13:23</span><br/>
You may enjoy this: nothing concrete yet but a vision for the future: "LLM As Compiler": <a href="https://medium.com/redsquirrel-tech/llm-as-compiler-2a2f79d30f0b">https://medium.com/redsquirrel-tech/llm-as-compiler-2a2f79d30f0b</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696884151.992949"></a>
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696884151.992949" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-09 13:42</span><br/>
Interesting, thanks <span style="background-color:#ccf">@Greg Bylenok</span>! So if I get this correctly, the idea is that users will do coding in high-level pseudo-language, compile it to the actual target language and then verify that it works as intended.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696929453.580749"></a>
      <img src="https://avatars.slack-edge.com/2021-03-12/1859691333940_e169f54bbaf8b9b36b12_72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696929453.580749" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Konrad Hinsen</b>
<span style="margin:2em; color:#606060">2023-10-10 02:17</span><br/>
Something I'd like to see (but don't expect to) is the use of controlled natural languages as the intermediate layer. Generated by an LLM, checked by a human. Controlled natural languages are hard to write but easy to read, so they look like just the right level.<br/>Problem: there is no big corpus of any controlled natural language that could be used to train LLMs. And such a corpus would be very expensive to create, because these languages are a pain to write.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696933279.121379"></a>
      <img src="https://secure.gravatar.com/avatar/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696933279.121379" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Janne Aukia</b>
<span style="margin:2em; color:#606060">2023-10-10 03:21</span><br/>
I've seen some posts about enforcing constraints on LLM output. Not sure what was the one I saw on Twitter, but perhaps this is something related: <a href="https://github.com/IsaacRe/Syntactically-Constrained-Sampling">https://github.com/IsaacRe/Syntactically-Constrained-Sampling</a><br/><br/>Something like this might work even for a controlled natural language, as long as you can formalize/validate the language?<br/><br/>And I really hope that some type of forced constraints will be in all LLM:s soon.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696937107.877229"></a>
      <img src="https://avatars.slack-edge.com/2023-06-13/5441522160256_154a9d12968ca5a13cf5_72.jpg" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696937107.877229" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Greg Bylenok</b>
<span style="margin:2em; color:#606060">2023-10-10 04:25</span><br/>
100% on the constraints. For now, we attempt to accomplish through prompting, like "put your answer in the following JSON structure"... When it neglects to do so, we have to remind it via "now put your answer in the following JSON structure". Obviously not robust, but so far "good enough" for our use case. Another change that would help: eliminating the non-determinism in the reply. Can anyone explain why today's LLMs are not deterministic? Has non-determinism been added as a "feature" to avoid repetitive responses? Or is it somehow fundamental to the inference algorithm itself?
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696937952.202249"></a>
      <img src="https://avatars.slack-edge.com/2021-03-12/1859691333940_e169f54bbaf8b9b36b12_72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696937952.202249" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Konrad Hinsen</b>
<span style="margin:2em; color:#606060">2023-10-10 04:39</span><br/>
It's a feature. And as long as LLM output is not reliable, I don't see the point of making it determinstic.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1696949882.249399"></a>
      <img src="https://avatars.slack-edge.com/2021-09-13/2508698086192_565c54a4fa91a0c8c75a_72.png" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1696949882.249399" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Tom Lieber</b>
<span style="margin:2em; color:#606060">2023-10-10 07:58</span><br/>
<span style="background-color:#ccf">@Greg Bylenok</span> <a href="https://writings.stephenwolfram.com/2023/05/the-new-world-of-llm-functions-integrating-llm-technology-into-the-wolfram-language/">https://writings.stephenwolfram.com/2023/05/the-new-world-of&hellip;</a> touches on it.<br/><br/>“The basic issue is that current neural nets operate with approximate real numbers, and occasionally roundoff in those numbers can be critical to “decisions” made by the neural net (typically because the application of the activation function for the neural net can lead to a bifurcation between results from numerically nearby values). And so, for example, if different LLMFunction evaluations happen on servers with different hardware and different roundoff characteristics, the results can be different.”<br/><br/>The next few paragraphs go into how GPU parallelism inserts nondeterminism too.<br/><br/>But that’s just an explanation of why it’s difficult to remove entirely. Most systems do insert it intentionally to produce variation in the responses.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1699314470.904839"></a>
      <img src="https://avatars.slack-edge.com/2023-06-13/5441522160256_154a9d12968ca5a13cf5_72.jpg" style="float:left"/>
      <a href="../thinking-together/1696845341.002639.html#1699314470.904839" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Greg Bylenok</b>
<span style="margin:2em; color:#606060">2023-11-06 15:47</span><br/>
OpenAI just announced new features related to a couple of concerns voiced in this thread. First, a new "seed" feature that adds some amount of determinism to the output. More here: <a href="https://platform.openai.com/docs/guides/text-generation/reproducible-outputs">https://platform.openai.com/docs/guides/text-generation/reproducible-outputs</a>. Second, a new JSON mode that guarantees the output is at least valid JSON: <a href="https://platform.openai.com/docs/guides/text-generation/json-mode">https://platform.openai.com/docs/guides/text-generation/json-mode</a>)
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
