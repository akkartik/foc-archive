<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #of-ai</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://avatars.slack-edge.com/2023-02-10/4782052692709_972d4c887a7c689aae4a_72.jpg" style="float:left"/>
      <a href="../of-ai/1745550502.455169.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Nilesh Trivedi</b>
<span style="margin:2em; color:#606060">2025-04-24 20:08</span><br/>
Hi all,<br/><br/>Have any of you run into questions where large AI models are missing crucial conceptual knowledge as well as are unable to find it by using Web search as a tool?<br/><br/>In other words, what are some examples of the blind spots of "AI + public Internet"? <br/><br/>I really mean CONCEPTUAL knowledge, i.e. HOW things work in the world, not mere factoids or events. Will likely be super-niche, or some nuance that has not been discussed on the Web, and therefore missing from the training data.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1745942747.962439"></a>
      <img src="https://secure.gravatar.com/avatar/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png" style="float:left"/>
      <a href="../of-ai/1745550502.455169.html#1745942747.962439" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>William Taysom</b>
<span style="margin:2em; color:#606060">2025-04-29 09:05</span><br/>
Questions where most of the corpus answers a slightly though materially different question.  Off the top of my head, back years ago now, we noticed anything resembling the monty-hall problem was assumed to be an instance thereof.  Reasoning models probably have fixed that one.  The other week though I ran into a little trouble asking demographic questions about how many children families have in different places and this generally being interpreted as basic fertility.  Of course a little clarification (sometimes in a fresh conversation) goes a long way.  Let's see... here's a good one.  Compare asking how many siblings do people in a place have vs if a woman has one child, how many other children does she have on average.  Actually, this makes for a good trick question, "Suppose we have demographic data telling us how many siblings each person in the sample has. What can we then say about how many children a given woman has?"
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1745947452.655669"></a>
      <img src="https://secure.gravatar.com/avatar/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png" style="float:left"/>
      <a href="../of-ai/1745550502.455169.html#1745947452.655669" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>William Taysom</b>
<span style="margin:2em; color:#606060">2025-04-29 10:24</span><br/>
Oh... Noticed this in Claude's system prompt... "If Claude is shown a classic puzzle, before proceeding, it quotes every constraint or premise from the person’s message word for word before inside quotation marks to confirm it’s not dealing with a new variant."
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
