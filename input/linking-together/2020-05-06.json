[
    {
        "user": "UE6EFEPTQ",
        "type": "message",
        "ts": "1588752114.182600",
        "client_msg_id": "e083cbdb-c75b-4e9d-b382-c92234c89c15",
        "text": "Mr Beads: do you have a reference for the throwing away temperature data? I presume you follow Tony Heller? (Apologies for derailing once again)",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "8073c43d5d8d",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-12-18\/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
            "first_name": "Duncan",
            "real_name": "Duncan Cragg",
            "display_name": "Duncan Cragg",
            "team": "T5TCAFTA9",
            "name": "fp",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1588433913.149100",
        "parent_user_id": "UJ6LDMMN0",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Wc72",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Mr Beads: do you have a reference for the throwing away temperature data? I presume you follow Tony Heller? (Apologies for derailing once again)"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "ok_hand",
                "users": [
                    "UJ6LDMMN0"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "U5STGTB3J",
        "type": "message",
        "ts": "1588760545.189200",
        "edited": {
            "user": "U5STGTB3J",
            "ts": "1588760760.000000"
        },
        "client_msg_id": "3C7837A1-0AA2-41CA-9B63-55D42C30A2AF",
        "text": "Bringing it half-way back to the original discussion: it seems that simulations get more scrutiny than written media — why is that? \n\nOn the one hand I can see that giving the reader\/user more authority to come to their own conclusions manifests as sort of a loss of control on the author\/developer side. In written media we seem to have accepted lower standards and let such things pass, often if at least some references have been given (but we don’t look at them to verify) or given the article comes from a “reputable” source (which means different things to different people, depending on their filter bubble).",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1588433913.149100",
        "parent_user_id": "UJ6LDMMN0",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "i=6",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Bringing it half-way back to the original discussion: it seems that simulations get more scrutiny than written media — why is that? \n\nOn the one hand I can see that giving the reader\/user more authority to come to their own conclusions manifests as sort of a loss of control on the author\/developer side. In written media we seem to have accepted lower standards and let such things pass, often if at least some references have been given (but we don’t look at them to verify) or given the article comes from a “reputable” source (which means different things to different people, depending on their filter bubble)."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "cake",
                "users": [
                    "UC2A2ARPT"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UEQ6M68H0",
        "type": "message",
        "ts": "1588786292.190200",
        "client_msg_id": "c8b4b816-5ccb-4c9e-a805-dfbb831ed1d0",
        "text": "<@UE6EFEPTQ> here is the NY times article on the actual temperature data being replaced by \"homogenized\" data. <https:\/\/archive.nytimes.com\/www.nytimes.com\/gwire\/2009\/10\/14\/14greenwire-scientists-return-fire-at-climate-skeptics-in-31175.html>\n\nThe source code to the UK model that caused billions of dollars in damage has been posted, and i saw a reddit thread of people who were plowing through the 50,000 lines of C++ (although written basically in C). There a key 5000 line long function that is the core of the model. Almost completely uncommented, and using plenty of single letter variable names, written over 10 years, this is not the kind of quality software you want to trust. The code for important govt. models should be open to scrutiny.\n\nThere are computer models that are accurate. For example, in Forestry, the *FORESEE** model predicts forest growth*.* The reason they are accurate is that the people running the models take their measurements over a long period of time (20+ years), and adjust their parameters so that the growth factors are accurate. And you try new sample areas and compare how the predictions worked out. The Foresee shows that if you take a forest and divide it into 100 equal area regions, and clear cut 2 of them, in 50 years you will have cycled through the forest, and this practice generates ultimately 20-30% more tree cover without any damage to wildlife. Optimization can be improved with computer models, but you have to take the time and effort to verify they are accurate.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "326328f75c3f",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-02-05\/542651515888_326328f75c3f2a08544c_72.jpg",
            "first_name": "Edward",
            "real_name": "Edward de Jong",
            "display_name": "Edward de Jong \/ Beads Project",
            "team": "T5TCAFTA9",
            "name": "magicmouse94937",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1588433913.149100",
        "parent_user_id": "UJ6LDMMN0",
        "attachments": [
            {
                "title": "Scientists Return Fire at Climate Skeptics in 'Destroyed Data' Dispute - NYTimes.com",
                "title_link": "https:\/\/archive.nytimes.com\/www.nytimes.com\/gwire\/2009\/10\/14\/14greenwire-scientists-return-fire-at-climate-skeptics-in-31175.html",
                "text": "Climate scientists are refuting claims that raw data used in critical climate change reports has been destroyed, rendering th...",
                "fallback": "Scientists Return Fire at Climate Skeptics in 'Destroyed Data' Dispute - NYTimes.com",
                "from_url": "https:\/\/archive.nytimes.com\/www.nytimes.com\/gwire\/2009\/10\/14\/14greenwire-scientists-return-fire-at-climate-skeptics-in-31175.html",
                "service_icon": "https:\/\/archive.nytimes.com\/favicon.ico",
                "service_name": "archive.nytimes.com",
                "id": 1,
                "original_url": "https:\/\/archive.nytimes.com\/www.nytimes.com\/gwire\/2009\/10\/14\/14greenwire-scientists-return-fire-at-climate-skeptics-in-31175.html"
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "pOGmB",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UE6EFEPTQ"
                            },
                            {
                                "type": "text",
                                "text": " here is the NY times article on the actual temperature data being replaced by \"homogenized\" data. "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/archive.nytimes.com\/www.nytimes.com\/gwire\/2009\/10\/14\/14greenwire-scientists-return-fire-at-climate-skeptics-in-31175.html"
                            },
                            {
                                "type": "text",
                                "text": "\n\nThe source code to the UK model that caused billions of dollars in damage has been posted, and i saw a reddit thread of people who were plowing through the 50,000 lines of C++ (although written basically in C). There a key 5000 line long function that is the core of the model. Almost completely uncommented, and using plenty of single letter variable names, written over 10 years, this is not the kind of quality software you want to trust. The code for important govt. models should be open to scrutiny.\n\nThere are computer models that are accurate. For example, in Forestry, the "
                            },
                            {
                                "type": "text",
                                "text": "FORESEE* ",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "model predicts forest growth"
                            },
                            {
                                "type": "text",
                                "text": ". ",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "The reason they are accurate is that the people running the models take their measurements over a long period of time (20+ years), and adjust their parameters so that the growth factors are accurate. And you try new sample areas and compare how the predictions worked out. The Foresee shows that if you take a forest and divide it into 100 equal area regions, and clear cut 2 of them, in 50 years you will have cycled through the forest, and this practice generates ultimately 20-30% more tree cover without any damage to wildlife. Optimization can be improved with computer models, but you have to take the time and effort to verify they are accurate."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1::skin-tone-3",
                "users": [
                    "UE6EFEPTQ"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UJ6LDMMN0",
        "type": "message",
        "ts": "1588787459.190500",
        "edited": {
            "user": "UJ6LDMMN0",
            "ts": "1588788134.000000"
        },
        "client_msg_id": "2af1f048-fff3-45b2-9060-7d9109c4a377",
        "text": "(sorry busy times, so I can't come here as often as I want even if this thread interests me a lot)",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0c210ee2df74",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-11-13\/2722434855730_0c210ee2df74838f8683_72.png",
            "first_name": "nicolas",
            "real_name": "nicolas decoster",
            "display_name": "ogadaki",
            "team": "T5TCAFTA9",
            "name": "nicolas.decoster",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1588433913.149100",
        "parent_user_id": "UJ6LDMMN0",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "jR2",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "(sorry busy times, so I can't come here as often as I want even if this thread interests me a lot)"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UJ6LDMMN0",
        "type": "message",
        "ts": "1588788032.190700",
        "client_msg_id": "f9c7e05a-25cf-4c92-89e2-f2d8188c7751",
        "text": "Ivan wrote:\n&gt; I'd say the formulas are just as important. But when you expose too much, it becomes less accessible, and thus less useful as a teaching\/learning tool. You don't need an encyclopedia entry to contain every possible detail in order for it to usefully convey critical info. So we need some other solution than just \"make sure everyone is fully stats-literate and give them all the data and formulas\". That doesn't scale.\nYes, care must be taken to reveal only the level of information (here formulas) that is useful at the level reading. But maybe there is way to do it progressively? At least easier than the code for the Nicky Case article, where the formulas are in the JavaScript code mixed with UI parts. Maybe first the graph, then the parameters, then high level formulas, then lower level formulas, with as much level as needed down to the approximations the \"last\" formulas are doing and explicitly stating that they are abstractions and what this means in the reading of the whole story.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0c210ee2df74",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-11-13\/2722434855730_0c210ee2df74838f8683_72.png",
            "first_name": "nicolas",
            "real_name": "nicolas decoster",
            "display_name": "ogadaki",
            "team": "T5TCAFTA9",
            "name": "nicolas.decoster",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1588433913.149100",
        "parent_user_id": "UJ6LDMMN0",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "INRF",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Ivan wrote:\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'd say the formulas are just as important. But when you expose too much, it becomes less accessible, and thus less useful as a teaching\/learning tool. You don't need an encyclopedia entry to contain every possible detail in order for it to usefully convey critical info. So we need some other solution than just \"make sure everyone is fully stats-literate and give them all the data and formulas\". That doesn't scale."
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Yes, care must be taken to reveal only the level of information (here formulas) that is useful at the level reading. But maybe there is way to do it progressively? At least easier than the code for the Nicky Case article, where the formulas are in the JavaScript code mixed with UI parts. Maybe first the graph, then the parameters, then high level formulas, then lower level formulas, with as much level as needed down to the approximations the \"last\" formulas are doing and explicitly stating that they are abstractions and what this means in the reading of the whole story."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]