<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #thinking-together</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://avatars.slack-edge.com/2023-02-06/4754627914258_41a8bada781281751d07_72.jpg" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Paul Tarvydas</b>
<span style="margin:2em; color:#606060">2025-01-16 06:34</span><br/>
As I understand it, to achieve true concurrency on a single computer, you need to ensure that app code sits in one of the core-private caches (L1, L2, but not L3). <a href="https://open.substack.com/pub/programmingsimplicity/p/practical-parallelism?r=1egdky&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true">Thoughts</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737054021.209679"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737054021.209679" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 11:00</span><br/>
that sounds like an untrue scotsman to me :stuck_out_tongue: but maybe you can clarify how you're defining "true concurrency"
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737058450.848719"></a>
      <img src="https://avatars.slack-edge.com/2023-02-06/4754627914258_41a8bada781281751d07_72.jpg" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737058450.848719" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Paul Tarvydas</b>
<span style="margin:2em; color:#606060">2025-01-16 12:14</span><br/>
<a href="https://www.google.com/search?client=safari&amp;sca_esv=540dfeac42c719c1&amp;rls=en&amp;sxsrf=ADLYWIKexny3Kjz-0EgeSN9lCkush8yiwA:1737055378868&amp;q=concurrency&amp;si=ACC90nwKPQWKXvO0LWGU61hOTgoDBwhKj56hDTZ5a9FkGxdB-eW_Wi-76RRz5uldqyDSNxGoBx2sNSDo9jWwpWVbYl5rpYamR9g1pxU3hFew6rSrD4pNHnY%3D&amp;expnd=1&amp;sa=X&amp;ved=2ahUKEwjP6b_G-_qKAxUSIjQIHdcDIBAQ2v4IegQIKBAb&amp;biw=1415&amp;bih=760">concurrency</a><br/>...the ability to execute more than one program or task simultaneously...<br/><br/>In a non-cache CPU, it never happens that tasks run at the same time. They only appear to do so because context-switching happens so fast as to fool us humans. There's only 1 CPU and it gets time-shared between threads.<br/><br/>In multi-core CPUs on the same chip, the distinction becomes blurred. If the code runs entirely in the private cache, with no cache misses, the code runs (truly) concurrently. Programs can run "at the same time", limited by the number of cores available. But, the cores block and wait if the code needs to access shared cache or shared main memory. The hardware determines the synchronization. The software code gets no say in this - it just tries to access memory and may get blocked by the hardware.<br/><br/>In my mind, L1/L2/L3 caching is just a kludge driven by 1950s desires to share memory on time-shared, single-threaded machines. These days, using bowls full of Arduinos would be a smarter choice if one wanted simple (true) concurrency. It seems to me that the software world is being presented with more and more complicated syntaxes for gluing asynchronousity on top of synchronous languages which run on top of already-asynchronous electronics.<br/><br/>FTR: I don't consider processes that take themselves out of the picture while waiting for async I/O to be "running" (synchronously, nor concurrently). Sitting in memory whilst executing no instructions is not "running" in my vocabulary. In contrast, operating systems bestow the state named "Running" to any process which isn't "Blocked", yet, might not actually own CPU time and is not executing opcodes. In my book, any process which is (truly) "running" needs to be in charge of a CPU (or a core). Another way to put it: if you have 1,000 processes and fewer than 1,000 cores then the best you can do is to simulate true concurrency. That's the basis of "threads" in all programming languages that I know about - simulation of concurrency, not concurrency.<br/><br/>In my mind the fundamental problem is that, by using hardware to do low-level sync at the memory-access level, we take design decisions out of software artchitects' hands. Something like very explicit message-passing would be better (not the Smalltalk kind of "message passing" but the internet kind of message passing). I just don't like hidden, under-the-hood, "surprise" blocking where some other process can determine my process' run-time. Hidden blocking is OK if you're building "calculators", but, not so OK if you're building internet-y (sequencing) software where you want total control/expression of all latencies and running times.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737058641.862119"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737058641.862119" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 12:17</span><br/>
&gt; In a non-cache CPU, it never happens that tasks run at the same time<br/>that's not necessarily true, though, right? there's nothing that <em>requires</em> a cache to allow concurrency, and there's not a fundamental reason you couldn't, it's just that they both tend to be present together in modern cpus
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737058783.070259"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737058783.070259" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 12:19</span><br/>
&gt; In my mind the fundamental problem is that, by using hardware to do low-level sync at the memory-access level, we take design decisions out of software artchitects' hands<br/>Wasn't this exactly the shift that approaches like itanium were trying for, and didn't get traction?
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737063413.554569"></a>
      <img src="https://avatars.slack-edge.com/2023-02-06/4754627914258_41a8bada781281751d07_72.jpg" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737063413.554569" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Paul Tarvydas</b>
<span style="margin:2em; color:#606060">2025-01-16 13:36</span><br/>
> ... nothing that requires a cache to allow concurrency ...<br/>Correct, but concurrency - by definition - <em>requires</em> separate CPUs. Caching is just an attempt at decoupling cores without actually using distributed CPUs, whilst continuing to do what we've always been doing...<br/><br/>"Concurrency" is just a mis-use of a word from the English language. It would be more accurate to call it "time-sharing".<br/><br/>> ... and didn't get traction? ...<br/>I think that MOP (message-oriented-programming) is necessarily on the horizon, due to the shift in our problem space, from 1950s single-threaded CPUs and building computation-based calculators to today's internet-y, robotics, IoT, etc. thrust.<br/><br/>I need to refresh my memory of what Itanium was attempting to do, but, I suspect that it tried to accommodate synchronous-language-think, which ain't the right way to approach asynchronous problems (and would explain failure ; on top of which, it was probably plagued by the "if we asked people what they wanted, they'd have said faster horses" effect).
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737068415.892669"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737068415.892669" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 15:00</span><br/>
&gt; Caching is just an attempt at decoupling cores without actually using distributed CPUs, whilst continuing to do what we've always been doing...<br/>I think we disagree there... I see that as a side-effect of caching, perhaps, but not at all fundamental to the need or implementation
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737068760.742189"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737068760.742189" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 15:06</span><br/>
&gt; "Concurrency" is just a mis-use of a word from the English language. It would be more accurate to call it "time-sharing".<br/>I think you're redefining what you're calling "true" concurrency. Sure, you can keep drilling down deeper in the stack in search of your truth, but at what point do you stop? I don't choose to stop calling something concurrent just because there's also a need to arbitrate access to shared hardware resources sometimes.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737068898.961849"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737068898.961849" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 15:08</span><br/>
I think there's a reason we've built these abstractions and choose to program on them, rather than try to count cycles and account for discrete hardware unit requirements at a software level
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737069241.813549"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737069241.813549" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 15:14</span><br/>
> but concurrency - by definition - requires separate CPUs<br/>And also don't agree with that on modern cpus, either... given pipelining has become pretty ubiquitous, each cpu/core is often also concurrently executing different stages of multiple instructions
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737069466.219969"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737069466.219969" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 15:17</span><br/>
(though again, there are limits to that imposed by the fundamental physical limits of the hardware, so again we let the hardware arbitrate that, and benefit from it when we can, but recognize that contention for resources can also cause pipeline stalls)
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737069581.047999"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737069581.047999" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 15:19</span><br/>
None of this fundamentally contradicts your conclusion that maybe we should be moving towards a message passing model, but I don't feel like this is a compelling argument for it in my opinion, either
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737092187.583759"></a>
      <img src="https://secure.gravatar.com/avatar/bc993d98fe7bf26c048ac0818a598d4d.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0001-72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737092187.583759" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Mark Dewing</b>
<span style="margin:2em; color:#606060">2025-01-16 21:36</span><br/>
The Cerebras wafer-scale processors have no shared access to memory (no cache).  Each core can only access it's local memory. Planning the message routes between individual cores becomes one of the challenging parts of programming it.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737092719.785889"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737092719.785889" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 21:45</span><br/>
yeah, I recall the early parallela manycore designs were similar... either the programmer, the tools, or some combination of both had to "understand" the implications of interconnects to work well. In the ideal case they could outperform "classic" models by a big margin, but in the more realistic case of "semi-skilled programmer throws code spaghetti at the naive port of gcc's backend" the results were <em>way</em> worse.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737092896.789009"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737092896.789009" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 21:48</span><br/>
ia64 was a different approach, it was much less about moving away from shared memory models, but it also was an attempt to shift the "understand hardware implications" to the software side
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1737092977.455669"></a>
      <img src="https://avatars.slack-edge.com/2024-11-24/8076913595155_ea40cf1a7c836d55cf1c_72.png" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1737092977.455669" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Andrew Beyer</b>
<span style="margin:2em; color:#606060">2025-01-16 21:49</span><br/>
and I suspect if the backing of the Intel/AMD rivalry wasn't sufficient funding to make that make sense... not going to say it's not possible, but also going to be very skeptical of any "quick fixes"
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1738060280.168939"></a>
      <img src="https://avatars.slack-edge.com/2023-02-06/4754627914258_41a8bada781281751d07_72.jpg" style="float:left"/>
      <a href="../thinking-together/1737038043.781959.html#1738060280.168939" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Paul Tarvydas</b>
<span style="margin:2em; color:#606060">2025-01-28 02:31</span><br/>
There seems to be an underlying belief that strip shedding an application and making the strips concurrent makes the whole application concurrent. I don't agree. <a href="https://open.substack.com/pub/programmingsimplicity/p/concurrency-and-strip-shredding?r=1egdky&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true">More thoughts</a>
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
