<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #of-ai</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://avatars.slack-edge.com/2023-02-10/4782052692709_972d4c887a7c689aae4a_72.jpg" style="float:left"/>
      <a href="../of-ai/1750049681.063769.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Nilesh Trivedi</b>
<span style="margin:2em; color:#606060">2025-06-15 21:54</span><br/>
>  AI is a new kind of computer.<br/>> • A traditional computer processes structured data with deterministic instructions.<br/>> • AI processes unstructured data with natural-language nondeterministic instructions.<br/>I like the simplicity of this framing.<br/><br/>But personally, I am more interested in unifying both these kind of computational work: Mathematical (precise & deterministic data structures and instructions) and human-media centric (language, image/audio/video etc) which approximate/ambiguous.<br/><br/><a href="https://jeffhuber.substack.com/p/ai-is-a-new-computer">https://jeffhuber.substack.com/p/ai-is-a-new-computer</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1750088516.074869"></a>
      <img src="https://avatars.slack-edge.com/2021-05-30/2111810970118_8c781271e0f034f7c468_72.jpg" style="float:left"/>
      <a href="../of-ai/1750049681.063769.html#1750088516.074869" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Arvind Thyagarajan</b>
<span style="margin:2em; color:#606060">2025-06-16 08:41</span><br/>
<em>"universal unstructured information processor that can simulate any intuitive procedure (reasoning) given sufficient resources and the proper context"</em><br/><br/>the "reasoning" part is called into question somewhat here: <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf">https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf</a><br/><br/>"despite sophisticated self-reflection mechanisms, these models fail to develop generalizable reasoning capabilities beyond certain complexity thresholds. We identified three distinct reasoning regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at moderate complexity, and both collapse at high complexity."<br/><br/>at one's most cynical one might be forgiven for walking away from this short paper with the conclusion that the current state of large models is a super-expensive super-sophisticated autocomplete? too cynical??
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1750089033.468409"></a>
      <img src="https://avatars.slack-edge.com/2023-02-10/4782052692709_972d4c887a7c689aae4a_72.jpg" style="float:left"/>
      <a href="../of-ai/1750049681.063769.html#1750089033.468409" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Nilesh Trivedi</b>
<span style="margin:2em; color:#606060">2025-06-16 08:50</span><br/>
I do believe the Apple paper is way too cynical. <br/><br/>Look up the benchmark score on GAIA leaderboard, then take a look at the sample tasks in the dataset. I bet you will find it ridiculous to call it a "sophisticated autocomplete" then.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1750089612.847559"></a>
      <img src="https://avatars.slack-edge.com/2021-05-30/2111810970118_8c781271e0f034f7c468_72.jpg" style="float:left"/>
      <a href="../of-ai/1750049681.063769.html#1750089612.847559" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Arvind Thyagarajan</b>
<span style="margin:2em; color:#606060">2025-06-16 09:00</span><br/>
oh no, they've co-opted gaia!! (<a href="https://en.wikipedia.org/wiki/Gaia">https://en.wikipedia.org/wiki/Gaia</a>)
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
