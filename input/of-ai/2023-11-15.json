[
    {
        "user": "U03LJBR6THT",
        "type": "message",
        "ts": "1700056012.366579",
        "client_msg_id": "35938d37-befc-4c97-abb0-7cecbbd7bf92",
        "text": "I don't know much about LLMs, so I'm purely asking out of ignorance.\n\nAFAIK Open AI has a powerful LLM system in large part due to large compute power.\n\nIs there a future where small business \/ people can compete with mega-corps in the LLM space, or will we be renting GPT (or similar) until end the of time?",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0afa5db0b2d5",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-07-06\/5524013698279_0afa5db0b2d593650747_72.jpg",
            "first_name": "Marcelle",
            "real_name": "Marcelle Rusu",
            "display_name": "Marcelle Rusu",
            "team": "T5TCAFTA9",
            "name": "marcelrusu0",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700056012.366579",
        "reply_count": 34,
        "reply_users_count": 5,
        "latest_reply": "1700287667.115419",
        "reply_users": [
            "U04JY2BF24E",
            "U03LJBR6THT",
            "UCUSW7WVD",
            "UJBAJNFLK",
            "U04KZ8A9WCT"
        ],
        "replies": [
            {
                "user": "U04JY2BF24E",
                "ts": "1700062161.813439"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700062673.590299"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700064852.035669"
            },
            {
                "user": "U04JY2BF24E",
                "ts": "1700064989.733659"
            },
            {
                "user": "UJBAJNFLK",
                "ts": "1700116117.161219"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700145843.555359"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700146012.005689"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700146015.776779"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700152316.072209"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700156875.083529"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700173852.623329"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700174009.484769"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700174328.392959"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700175405.266649"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700175622.959999"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700175655.444159"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700175662.400949"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700175687.672539"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700175727.041449"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700175748.355399"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700175813.963829"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700176014.842079"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700176381.132869"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700176407.770299"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700187010.226299"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700191484.938339"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700196076.081899"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700196262.922459"
            },
            {
                "user": "UJBAJNFLK",
                "ts": "1700224918.961809"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700231820.135709"
            },
            {
                "user": "U04KZ8A9WCT",
                "ts": "1700232724.257009"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700232880.448479"
            },
            {
                "user": "U03LJBR6THT",
                "ts": "1700233194.868759"
            },
            {
                "user": "UJBAJNFLK",
                "ts": "1700287667.115419"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "q0Euw",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I don't know much about LLMs, so I'm purely asking out of ignorance.\n\nAFAIK Open AI has a powerful LLM system in large part due to large compute power.\n\nIs there a future where small business \/ people can compete with mega-corps in the LLM space, or will we be renting GPT (or similar) until end the of time?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03LJBR6THT",
        "type": "message",
        "ts": "1700059749.516609",
        "client_msg_id": "001ea4b3-313b-4568-b2dc-6f7679009c55",
        "text": "I really enjoyed this talk, thanks for sharing",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0afa5db0b2d5",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-07-06\/5524013698279_0afa5db0b2d593650747_72.jpg",
            "first_name": "Marcelle",
            "real_name": "Marcelle Rusu",
            "display_name": "Marcelle Rusu",
            "team": "T5TCAFTA9",
            "name": "marcelrusu0",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1699618834.267679",
        "parent_user_id": "U03R0B9U1GD",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "SrzGO",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I really enjoyed this talk, thanks for sharing"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "raised_hands",
                "users": [
                    "U03R0B9U1GD"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "U04JY2BF24E",
        "type": "message",
        "ts": "1700062161.813439",
        "edited": {
            "user": "U04JY2BF24E",
            "ts": "1700062285.000000"
        },
        "client_msg_id": "35bf72c5-7ce1-4d99-a9d3-3c7fb1d85d8b",
        "text": "In the immediate future, likely no. As LLMs get bigger and bigger, the computing power grows non-linearly. We may reach a point where general intelligence is \"good enough\" for most tasks, and then model expansion is deemed uneconomical. We are no where near that point, though.\n\nStartups and individuals are more likely to compete in domain-specific areas that don't require general intelligence and don't benefit from expanded context windows.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "154a9d12968c",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-06-13\/5441522160256_154a9d12968ca5a13cf5_72.jpg",
            "first_name": "Greg",
            "real_name": "Greg Bylenok",
            "display_name": "Greg Bylenok",
            "team": "T5TCAFTA9",
            "name": "gregory.bylenok",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700056012.366579",
        "parent_user_id": "U03LJBR6THT",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3ayJA",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "In the immediate future, likely no. As LLMs get bigger and bigger, the computing power grows non-linearly. We may reach a point where general intelligence is \"good enough\" for most tasks, and then model expansion is deemed uneconomical. We are no where near that point, though.\n\nStartups and individuals are more likely to compete in domain-specific areas that don't require general intelligence and don't benefit from expanded context windows."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03LJBR6THT",
        "type": "message",
        "ts": "1700062673.590299",
        "client_msg_id": "f1077693-5dcf-4d7b-bd87-dbef94539ad6",
        "text": "&gt; We may reach a point where general intelligence is \"good enough\" for most tasks, and then model expansion is deemed uneconomical\nSo I'm clear, in this future when a model is good enough, can I go ahead &amp; run it myself on my computer? And how does it solve my concern?",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0afa5db0b2d5",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-07-06\/5524013698279_0afa5db0b2d593650747_72.jpg",
            "first_name": "Marcelle",
            "real_name": "Marcelle Rusu",
            "display_name": "Marcelle Rusu",
            "team": "T5TCAFTA9",
            "name": "marcelrusu0",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700056012.366579",
        "parent_user_id": "U03LJBR6THT",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "1shm7",
                "elements": [
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "We may reach a point where general intelligence is \"good enough\" for most tasks, and then model expansion is deemed uneconomical"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nSo I'm clear, in this future when a model is good enough, can I go ahead & run it myself on my computer? And how does it solve my concern?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UCUSW7WVD",
        "type": "message",
        "ts": "1700064852.035669",
        "client_msg_id": "5e400e0f-ae99-410a-abb6-4b0b17d4582d",
        "text": "It feels like an open question. Like in any negotiation, companies will reasonably try to get what they can get away with. So consumers will have to vote with their dollars and adoption for models they can run locally. So far there are good signs that open source models might keep up.\n\nOn compute requirements, two points:\n1. The bulk of compute is needed for training.\n2. There is definite economic pressure for training to yield efficient models that can run on phones.\nSo I don't think we'll be constrained by laws of physics here, only the laws of geopolitics.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "6e649a383cf8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-07-14\/687915485201_6e649a383cf8f9e366e3_72.png",
            "first_name": "Kartik",
            "real_name": "Kartik Agaram",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "ak",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700056012.366579",
        "parent_user_id": "U03LJBR6THT",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "nZ9zC",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "It feels like an open question. Like in any negotiation, companies will reasonably try to get what they can get away with. So consumers will have to vote with their dollars and adoption for models they can run locally. So far there are good signs that open source models might keep up.\n\nOn compute requirements, two points:\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "The bulk of compute is needed for training."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "There is definite economic pressure for training to yield efficient models that can run on phones."
                                    }
                                ]
                            }
                        ],
                        "style": "ordered",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "So I don't think we'll be constrained by laws of physics here, only the laws of geopolitics."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04JY2BF24E",
        "type": "message",
        "ts": "1700064989.733659",
        "client_msg_id": "f5999904-48c4-4059-a5de-9c2298282080",
        "text": "Re: \"can I run locally\": Yes. Individual's are currently running FB's open-source model (Llama2) on high-end personal hardware (say $10K) and just waiting for output. If Llama2 is \"good enough\" for your use case, then give it a few years for the hardware to catch up and allow for near-realtime inference. Look at the advancements in GPUs since the 1990s. Look at how mobile phones have evolved since the first iPhone. I wouldn't bet against similar happening here.  However, in the meantime, companies like OpenAI and Cohere are expanding not only the model sizes but also the \"context windows\" available to their models. (early 2023: like 32k tokens. today: 100K tokens). They not only have the investment dollars to build out this infrastructure, but they can also monetize that cost over their entire customer base. An individual or small startup is at a huge disadvantage there.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "154a9d12968c",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-06-13\/5441522160256_154a9d12968ca5a13cf5_72.jpg",
            "first_name": "Greg",
            "real_name": "Greg Bylenok",
            "display_name": "Greg Bylenok",
            "team": "T5TCAFTA9",
            "name": "gregory.bylenok",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700056012.366579",
        "parent_user_id": "U03LJBR6THT",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "5aL2h",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Re: \"can I run locally\": Yes. Individual's are currently running FB's open-source model (Llama2) on high-end personal hardware (say $10K) and just waiting for output. If Llama2 is \"good enough\" for your use case, then give it a few years for the hardware to catch up and allow for near-realtime inference. Look at the advancements in GPUs since the 1990s. Look at how mobile phones have evolved since the first iPhone. I wouldn't bet against similar happening here.  However, in the meantime, companies like OpenAI and Cohere are expanding not only the model sizes but also the \"context windows\" available to their models. (early 2023: like 32k tokens. today: 100K tokens). They not only have the investment dollars to build out this infrastructure, but they can also monetize that cost over their entire customer base. An individual or small startup is at a huge disadvantage there."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "100",
                "users": [
                    "UCUSW7WVD"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UJBAJNFLK",
        "type": "message",
        "ts": "1700116117.161219",
        "client_msg_id": "ea5283ed-e8dc-4781-b093-ab4ceb7b20ee",
        "text": "To complement <@UCUSW7WVD>'s observations, there's an interesting middle ground between training and using, and that is specializing pre-trained models for a specific domain. I expect this to become more important as it is so far the best path we have to obtaining more reliable output from LLMs. Open Source models could well become highly competitive if specialization can be done at low computational cost.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "e169f54bbaf8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-03-12\/1859691333940_e169f54bbaf8b9b36b12_72.png",
            "first_name": "Konrad",
            "real_name": "Konrad Hinsen",
            "display_name": "Konrad Hinsen",
            "team": "T5TCAFTA9",
            "name": "konrad.hinsen",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700056012.366579",
        "parent_user_id": "U03LJBR6THT",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "IXOdg",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "To complement "
                            },
                            {
                                "type": "user",
                                "user_id": "UCUSW7WVD"
                            },
                            {
                                "type": "text",
                                "text": "'s observations, there's an interesting middle ground between training and using, and that is specializing pre-trained models for a specific domain. I expect this to become more important as it is so far the best path we have to obtaining more reliable output from LLMs. Open Source models could well become highly competitive if specialization can be done at low computational cost."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]