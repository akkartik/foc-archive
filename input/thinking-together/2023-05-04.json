[
    {
        "user": "U79HM6726",
        "type": "message",
        "ts": "1683199124.327219",
        "client_msg_id": "854557b3-c9c5-413e-bcf3-2db66d111717",
        "text": "<@UA14TGLTC> If you ask it how many faces a shape consisting of two attached identical cubes has it will say 12 rather than 6. It’s understanding is very very limited and basic, almost as if it’s just advanced pattern-matching without any true deep understanding.\nIf classifying by Machiavelli’s classes of intellect (<https:\/\/www.goodreads.com\/quotes\/241451-because-there-are-three-classes-of-intellects-one-which-comprehends>), I haven’t seen it demonstrating even the second class (being able to distinguish whether something makes sense or not)",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g4ac39488e9c",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/24ac39488e9c4f2833d087a1343707dc.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Yair Chuchem",
            "display_name": "yairchu",
            "team": "T5TCAFTA9",
            "name": "yairchu",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679642239.661619",
        "parent_user_id": "UA14TGLTC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "HB9",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UA14TGLTC"
                            },
                            {
                                "type": "text",
                                "text": " If you ask it how many faces a shape consisting of two attached identical cubes has it will say 12 rather than 6. It’s understanding is very very limited and basic, almost as if it’s just advanced pattern-matching without any true deep understanding.\nIf classifying by Machiavelli’s classes of intellect ("
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.goodreads.com\/quotes\/241451-because-there-are-three-classes-of-intellects-one-which-comprehends"
                            },
                            {
                                "type": "text",
                                "text": "), I haven’t seen it demonstrating even the second class (being able to distinguish whether something makes sense or not)"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U016VUZGUUQ",
        "type": "message",
        "ts": "1683230960.602079",
        "edited": {
            "user": "U016VUZGUUQ",
            "ts": "1683231100.000000"
        },
        "client_msg_id": "9f3b87f7-5841-4695-a6ae-53e7b1f640e2",
        "text": "I don't think that's what Machiavelli is getting at in that quote, and also I have seen examples of GPT-X saying it's confused by a question and giving up, i.e. saying it doesn't make sense. There are examples of it exhibiting something very like a physical model of the world. Also, none of that addresses William's question. Before we can judge to what extent a machine is generating insight, we need to figure out what insight is.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gaee3c99144d",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/aee3c99144dfc6644c6c1f1303683140.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
            "first_name": "",
            "real_name": "Andrew F",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "andrewflnr",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679642239.661619",
        "parent_user_id": "UA14TGLTC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "z\/5Ey",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I don't think that's what Machiavelli is getting at in that quote, and also I have seen examples of GPT-X saying it's confused by a question and giving up, i.e. saying it doesn't make sense. There are examples of it exhibiting something very like a physical model of the world. Also, none of that addresses William's question. Before we can judge to what extent a machine is generating insight, we need to figure out what insight is."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U016VUZGUUQ",
        "type": "message",
        "ts": "1683231431.915559",
        "client_msg_id": "087d58db-c266-4ed6-8db5-47066b74d058",
        "text": "I think insight has to be something close to generalization: in many situations, find a common pattern or rule. It probably has to be predictive, if only probabilistically... It really is hard to tease apart from intuitive pattern matching. If you, a human, can predict what some weird system or situation will do, is it because you have insight or just because you're used to the pattern? Maybe it has to do with his explicit (communicable?) the mental model is.... It will not be easy to tell when\/if AI systems cross the line into \"insight\", assuming they haven't already.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gaee3c99144d",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/aee3c99144dfc6644c6c1f1303683140.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
            "first_name": "",
            "real_name": "Andrew F",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "andrewflnr",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679642239.661619",
        "parent_user_id": "UA14TGLTC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "H1Q",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I think insight has to be something close to generalization: in many situations, find a common pattern or rule. It probably has to be predictive, if only probabilistically... It really is hard to tease apart from intuitive pattern matching. If you, a human, can predict what some weird system or situation will do, is it because you have insight or just because you're used to the pattern? Maybe it has to do with his explicit (communicable?) the mental model is.... It will not be easy to tell when\/if AI systems cross the line into \"insight\", assuming they haven't already."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U5STGTB3J",
        "type": "message",
        "ts": "1683232897.013519",
        "client_msg_id": "10018B01-3861-494B-88C1-95CD3FB13BB2",
        "text": "We (and by that I mean cognitive science) has a pretty good grasp on what insight is at this point. And by that standard LLMs are certainly very far away from that.\n\nCan’t look it up right now, but I have a post on my Substack (which doesn’t explain insight, just tangentially mentions it), which links to an episode of Awakening from the Meaning Crisis which is about attention, insight, and flow and refers to other source material.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679642239.661619",
        "parent_user_id": "UA14TGLTC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3LYNd",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "We (and by that I mean cognitive science) has a pretty good grasp on what insight is at this point. And by that standard LLMs are certainly very far away from that.\n\nCan’t look it up right now, but I have a post on my Substack (which doesn’t explain insight, just tangentially mentions it), which links to an episode of Awakening from the Meaning Crisis which is about attention, insight, and flow and refers to other source material"
                            },
                            {
                                "type": "text",
                                "text": "."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]