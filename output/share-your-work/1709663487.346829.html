<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #share-your-work</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://secure.gravatar.com/avatar/2cd3e6c4c0086631a3e4d8f4c2b1f539.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0007-72.png" style="float:left"/>
      <a href="../share-your-work/1709663487.346829.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Luifer De Pombo</b>
<span style="margin:2em; color:#606060">2024-03-05 10:31</span><br/>
sharing some recent thoughts I have had about verifying LLM-generated bugfixes automatically with cloud infrastructure: <a href="https://lfdepombo.com/cloudbugfix">https://lfdepombo.com/cloudbugfix</a>. Today we validate LLM-generated code by looking at it or manually running it within our codebase. However if the expected behavior of the code is verifiable, there is a less painful workflow where the mistakes made by the LLM are not visible to us.
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
