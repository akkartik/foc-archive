[
    {
        "user": "UEQ7QL15F",
        "type": "message",
        "ts": "1700837030.649029",
        "edited": {
            "user": "UEQ7QL15F",
            "ts": "1700837040.000000"
        },
        "client_msg_id": "c800bcdb-16dd-4287-9d2c-12bf301056dd",
        "text": "I think that often the most difficult part of programming is debugging: humans aren't very good at \"seeing\" how an app operated when it failed. You get some log output that might or might not have references to the correct file locations.\n\nI don't see any reason, in the long term, why humans would be better than machines at debugging. How to make that happen? I assume somebody is building this already. Would it help if an AI with a large context window + access to the VM could see the whole call logs\/tree and see exactly what is going on?  AI could learn from other users, see everything that happens in a run without debugger\/console.logs, try multiple solutions in parallel, and fix issues while you sleep.\n\nThoughts on this?",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g52d221ae708",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/a52d221ae708f36674644a348005633a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0002-72.png",
            "first_name": "Janne",
            "real_name": "Janne Aukia",
            "display_name": "jaukia",
            "team": "T5TCAFTA9",
            "name": "janne.aukia",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "reply_count": 16,
        "reply_users_count": 8,
        "latest_reply": "1700992684.232459",
        "reply_users": [
            "UCUSW7WVD",
            "UKJGU23KP",
            "UE6EFEPTQ",
            "UE1JQM9HQ",
            "UJBAJNFLK",
            "UC2A2ARPT",
            "U0166ETPH61",
            "U016VUZGUUQ"
        ],
        "replies": [
            {
                "user": "UCUSW7WVD",
                "ts": "1700838146.560819"
            },
            {
                "user": "UKJGU23KP",
                "ts": "1700845438.775949"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700846323.336329"
            },
            {
                "user": "UKJGU23KP",
                "ts": "1700850005.253759"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1700852130.328389"
            },
            {
                "user": "UE6EFEPTQ",
                "ts": "1700869316.785959"
            },
            {
                "user": "UE1JQM9HQ",
                "ts": "1700902191.504349"
            },
            {
                "user": "UE6EFEPTQ",
                "ts": "1700904931.823659"
            },
            {
                "user": "UJBAJNFLK",
                "ts": "1700931987.738469"
            },
            {
                "user": "UC2A2ARPT",
                "ts": "1700934023.595229"
            },
            {
                "user": "U0166ETPH61",
                "ts": "1700949717.793379"
            },
            {
                "user": "UKJGU23KP",
                "ts": "1700953116.711729"
            },
            {
                "user": "UKJGU23KP",
                "ts": "1700953192.273329"
            },
            {
                "user": "U016VUZGUUQ",
                "ts": "1700963832.047319"
            },
            {
                "user": "UE1JQM9HQ",
                "ts": "1700967803.133219"
            },
            {
                "user": "UJBAJNFLK",
                "ts": "1700992684.232459"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Jpinl",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I think that often the most difficult part of programming is debugging: humans aren't very good at \"seeing\" how an app operated when it failed. You get some log output that might or might not have references to the correct file locations.\n\nI don't see any reason, in the long term, why humans would be better than machines at debugging. How to make that happen? I assume somebody is building this already. Would it help if an AI with a large context window + access to the VM could see the whole call logs\/tree and see exactly what is going on?  AI could learn from other users, see everything that happens in a run without debugger\/console.logs, try multiple solutions in parallel, and fix issues while you sleep.\n\nThoughts on this?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UCUSW7WVD",
        "type": "message",
        "ts": "1700838146.560819",
        "client_msg_id": "38ab7707-5676-4853-b9d7-1b5fcee3ccc1",
        "text": "In principle, LLMs could help here if they make reliable inferences and stop hallucinating facts. It'll be interesting to see how far those preconditions are achieved. Without them, you'd constantly have to debug your debugger.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "6e649a383cf8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-07-14\/687915485201_6e649a383cf8f9e366e3_72.png",
            "first_name": "Kartik",
            "real_name": "Kartik Agaram",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "ak",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "parent_user_id": "UEQ7QL15F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "16P8i",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "In principle, LLMs could help here if they make reliable inferences and stop hallucinating facts. It'll be interesting to see how far those preconditions are achieved. Without them, you'd constantly have to debug your debugger."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UKJGU23KP",
        "type": "message",
        "ts": "1700845438.775949",
        "client_msg_id": "9496C7B1-3603-4136-B574-0D316DB567F2",
        "text": "A lot of the times I’m most comfortable with LLMs is where I can easily check their output. Debugging is often going to work like that. ",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gfceba60ff0c",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/6fceba60ff0c90dc32cbff29054b02c2.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Justin Blank",
            "display_name": "Justin Blank",
            "team": "T5TCAFTA9",
            "name": "justin.blank",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "parent_user_id": "UEQ7QL15F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "8RuDO",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "A lot of the times "
                            },
                            {
                                "type": "text",
                                "text": "I’m"
                            },
                            {
                                "type": "text",
                                "text": " most comfortable with LLMs is where I can easily check their output. Debugging is often going to work like that. "
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "100",
                "users": [
                    "UCUSW7WVD",
                    "UEQ7QL15F",
                    "UC2A2ARPT",
                    "U05UBCXHWM6"
                ],
                "count": 4
            }
        ]
    },
    {
        "user": "UCUSW7WVD",
        "type": "message",
        "ts": "1700846323.336329",
        "client_msg_id": "2413ac1e-daba-4d15-b070-5a40565ce6af",
        "text": "I agree with your first sentence but not your second. I've often made a bugfix that addressed the _specific_ scenario I was repeatedly manually testing without understanding why, and as a result not fixing some other situations (and often breaking additional ones). My \"explanations\" can be rationalizations. Debugging is a process of understanding. You can't really judge if a bug has been fixed without understanding why it happened.\n\nMy suspicion is that LLMs will be really good at making the \"letter\" of arbitrary tests pass without quite meeting the \"spirit\" of the tests.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "6e649a383cf8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-07-14\/687915485201_6e649a383cf8f9e366e3_72.png",
            "first_name": "Kartik",
            "real_name": "Kartik Agaram",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "ak",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "parent_user_id": "UEQ7QL15F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "VYlS8",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I agree with your first sentence but not your second. I've often made a bugfix that addressed the "
                            },
                            {
                                "type": "text",
                                "text": "specific",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " scenario I was repeatedly manually testing without understanding why, and as a result not fixing some other situations (and often breaking additional ones). My \"explanations\" can be rationalizations. Debugging is a process of understanding. You can't really judge if a bug has been fixed without understanding why it happened.\n\nMy suspicion is that LLMs will be really good at making the \"letter\" of arbitrary tests pass without quite meeting the \"spirit\" of the tests."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "pray",
                "users": [
                    "URKQXRCAC"
                ],
                "count": 1
            },
            {
                "name": "heavy_plus_sign",
                "users": [
                    "U05BRNRAC4V"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UKJGU23KP",
        "type": "message",
        "ts": "1700850005.253759",
        "client_msg_id": "13DF2F6D-9D0E-494F-8F4B-A5A842464968",
        "text": "I think I agree, but I meant “if you had an LLM where it frequently gave the right fix a significant fraction of the time.” I think that’s a high bar, that they’re not close to meeting yet. \n\nBut in my response, I didn’t mean “check” merely as in “the tests pass”, but that you understand the change. \n\nAnd I think that in that sense, it’s true that you could imagine working with a not fully reliable AI and getting benefits from it. \n\nLeaving aside truly trivial bugs, I think in the majority of cases I work, finding the fix will take me noticeably longer than understanding why a candidate fix works. That’s why I can code review a coworker’s fix faster than I can make it myself (even if they don’t comment their fix). \n\nSo I think the target isn’t “you trust this checking things into your codebase”, but “can often find the offending line\/offer a fix that you can take or leave.”",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gfceba60ff0c",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/6fceba60ff0c90dc32cbff29054b02c2.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Justin Blank",
            "display_name": "Justin Blank",
            "team": "T5TCAFTA9",
            "name": "justin.blank",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "parent_user_id": "UEQ7QL15F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "mg+vy",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I think I agree, but I meant “if you had an LLM where it frequently gave the right fix a significant fraction of the time.” I think "
                            },
                            {
                                "type": "text",
                                "text": "that’s"
                            },
                            {
                                "type": "text",
                                "text": " a high bar, that "
                            },
                            {
                                "type": "text",
                                "text": "they’re"
                            },
                            {
                                "type": "text",
                                "text": " not close to meeting yet. \n\nBut in my response, I "
                            },
                            {
                                "type": "text",
                                "text": "didn’t"
                            },
                            {
                                "type": "text",
                                "text": " mean “check” merely as in “the tests pass”, but that you understand the change. \n\nAnd I think that in that sense, "
                            },
                            {
                                "type": "text",
                                "text": "it’s"
                            },
                            {
                                "type": "text",
                                "text": " true that you could imagine working with a not fully reliable AI and getting benefits from it. \n\nLeaving aside truly trivial bugs, I think in the majority of cases I work, finding the fix will take me noticeably longer than understanding why a candidate fix works. "
                            },
                            {
                                "type": "text",
                                "text": "That’s"
                            },
                            {
                                "type": "text",
                                "text": " why I can code review a coworker’s fix faster than I can make it myself (even if they "
                            },
                            {
                                "type": "text",
                                "text": "don’t"
                            },
                            {
                                "type": "text",
                                "text": " comment their fix). \n\nSo I think the "
                            },
                            {
                                "type": "text",
                                "text": "target isn’t"
                            },
                            {
                                "type": "text",
                                "text": " “you trust this checking things into your codebase”, but “can often find the offending line\/offer a fix that you can take or leave.”"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UCUSW7WVD",
        "type": "message",
        "ts": "1700852130.328389",
        "edited": {
            "user": "UCUSW7WVD",
            "ts": "1700852261.000000"
        },
        "client_msg_id": "6602356c-ec5c-451b-a881-9e07f3d60583",
        "text": "I see what you mean. The open question here seems to be about AI \"pragmatics\" in the sense of <https:\/\/en.wikiversity.org\/wiki\/Semantics_vs_pragmatics>. Perhaps I'm airing incompetence here, but I seldom check my coworkers' _logic_ in PRs. I mostly check product concerns (are we building the right thing?), process concerns (e.g. are the right things documented?) and architecture (does this PR change roughly the places I would expect?). For logic we rely on tests, and I build up trust over time in the people (entities) I work with (or give feedback to try to gain trust).\n\nI worry that an AI might be good at slipping through these heuristics of mine while performing its (automated or manual) tests in an antagonistic manner. Will it share my values and those of the broader culture I'm embedded in? Can I build up confidence over time that it shares my values, the way I can for the people I work with. Perhaps I'm still prejudiced because I don't live cheek by jowl with AIs yet :sweat_smile:",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "6e649a383cf8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-07-14\/687915485201_6e649a383cf8f9e366e3_72.png",
            "first_name": "Kartik",
            "real_name": "Kartik Agaram",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "ak",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "parent_user_id": "UEQ7QL15F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "PAfzi",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I see what you mean. The open question here seems to be about AI \"pragmatics\" in the sense of "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/en.wikiversity.org\/wiki\/Semantics_vs_pragmatics"
                            },
                            {
                                "type": "text",
                                "text": ". Perhaps I'm airing incompetence here, but I seldom check my coworkers' "
                            },
                            {
                                "type": "text",
                                "text": "logic",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " in PRs. I mostly check product concerns (are we building the right thing?), process concerns (e.g. are the right things documented?) and architecture (does this PR change roughly the places I would expect?). For logic we rely on tests, and I build up trust over time in the people (entities) I work with (or give feedback to try to gain trust).\n\nI worry that an AI might be good at slipping through these heuristics of mine while performing its (automated or manual) tests in an antagonistic manner. Will it share my values and those of the broader culture I'm embedded in? Can I build up confidence over time that it shares my values, the way I can for the people I work with. Perhaps I'm still prejudiced because I don't live cheek by jowl with AIs yet "
                            },
                            {
                                "type": "emoji",
                                "name": "sweat_smile",
                                "unicode": "1f605"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "thinking_face",
                "users": [
                    "UEQ7QL15F"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UE6EFEPTQ",
        "type": "message",
        "ts": "1700869316.785959",
        "client_msg_id": "a90f4734-7178-4c5e-9b31-377085ec77b0",
        "text": "I was about to flippantly say that I'm best at debugging when I've had a solid drink of beer or wine to start me off. But in fact, that's what is needed, human or AI: to take away the personal commitment to what you've done, and got wrong.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "8073c43d5d8d",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-12-18\/508431502471_8073c43d5d8dd3d3b4b2_72.jpg",
            "first_name": "Duncan",
            "real_name": "Duncan Cragg",
            "display_name": "Duncan Cragg",
            "team": "T5TCAFTA9",
            "name": "fp",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1700837030.649029",
        "parent_user_id": "UEQ7QL15F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "MxEJm",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I was about to flippantly say that I'm best at debugging when I've had a solid drink of beer or wine to start me off. But in fact, that's what is needed, human or AI: to take away the personal commitment to what you've done, and got wrong."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]