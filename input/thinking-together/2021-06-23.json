[
    {
        "user": "UCGAK10LS",
        "type": "message",
        "ts": "1624437161.147900",
        "edited": {
            "user": "UCGAK10LS",
            "ts": "1624440273.000000"
        },
        "client_msg_id": "7fd08576-5ce2-4c93-b034-d5f148d3088f",
        "text": "All modern programming languages apart from Rust (and I guess Swift, with its reference-counting) rely on garbage collection: a \"background thread\" locates memory the process has _forgotten about_ and marks it as available for re-use. However, this doesn't seem to be a sensible scheme in a distributed system where multiple processing devices each have local memories. That begs the question: if you want to design a programming language that can be transparently distributed over multiple devices, does it need to have a fancy type system (like Rust's) that enforces correct manual memory management?\n\nOne reason I'm thinking about this: most upcoming AI chips are using a \"network-on-chip\" architecture, which could also be called a \"distributed system on a chip\". A garbage collection algorithm on these chips would have to involve a message-passing protocol wherein different parts of the chip communicate to identify _forgotten_ memory. This seems like an unnecessarily complicated and expensive approach to memory management.\n\nThoughts? :unicorn_face:",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "6402e9775ed7",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-04-13\/5095853045814_6402e9775ed73b75334f_72.jpg",
            "first_name": "",
            "real_name": "Nick Smith",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "nmsmith65",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "reply_count": 24,
        "reply_users_count": 11,
        "latest_reply": "1625738442.201600",
        "reply_users": [
            "U5STGTB3J",
            "UBN9AFS0N",
            "U01PGQQEU2Z",
            "UA14TGLTC",
            "UT60XSVCN",
            "UCGAK10LS",
            "U025PBD75TM",
            "U013ZLJARC7",
            "U01T2EYBH0T",
            "USJ9LD0E4",
            "U027P92A0N5"
        ],
        "replies": [
            {
                "user": "U5STGTB3J",
                "ts": "1624441675.148100"
            },
            {
                "user": "UBN9AFS0N",
                "ts": "1624451568.148300"
            },
            {
                "user": "UBN9AFS0N",
                "ts": "1624451663.148500"
            },
            {
                "user": "U01PGQQEU2Z",
                "ts": "1624457980.148800"
            },
            {
                "user": "UA14TGLTC",
                "ts": "1624463680.149000"
            },
            {
                "user": "UT60XSVCN",
                "ts": "1624473008.149300"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1624496233.149800"
            },
            {
                "user": "U5STGTB3J",
                "ts": "1624518279.165800"
            },
            {
                "user": "U025PBD75TM",
                "ts": "1624541626.166500"
            },
            {
                "user": "U013ZLJARC7",
                "ts": "1624545723.167300"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1624575554.167800"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1624576469.168200"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1624576618.168400"
            },
            {
                "user": "U01T2EYBH0T",
                "ts": "1624746824.171400"
            },
            {
                "user": "U01T2EYBH0T",
                "ts": "1624747117.171600"
            },
            {
                "user": "U01T2EYBH0T",
                "ts": "1624747824.171900"
            },
            {
                "user": "USJ9LD0E4",
                "ts": "1624839859.172200"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1624849230.172400"
            },
            {
                "user": "U027P92A0N5",
                "ts": "1625727819.193300"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1625733652.193900"
            },
            {
                "user": "U027P92A0N5",
                "ts": "1625737221.194100"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1625737680.199100"
            },
            {
                "user": "UCGAK10LS",
                "ts": "1625737778.201400"
            },
            {
                "user": "U027P92A0N5",
                "ts": "1625738442.201600"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "HLJCj",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "All modern programming languages apart from Rust (and I guess Swift, with its reference-counting) rely on garbage collection: a \"background thread\" locates memory the process has "
                            },
                            {
                                "type": "text",
                                "text": "forgotten about",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " and marks it as available for re-use. However, this doesn't seem to be a sensible scheme in a distributed system where multiple processing devices each have local memories. That begs the question: if you want to design a programming language that can be transparently distributed over multiple devices, does it need to have a fancy type system (like Rust's) that enforces correct manual memory management?\n\nOne reason I'm thinking about this: most upcoming AI chips are using a \"network-on-chip\" architecture, which could also be called a \"distributed system on a chip\". A garbage collection algorithm on these chips would have to involve a message-passing protocol wherein different parts of the chip communicate to identify "
                            },
                            {
                                "type": "text",
                                "text": "forgotten",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " memory. This seems like an unnecessarily complicated and expensive approach to memory management.\n\nThoughts? "
                            },
                            {
                                "type": "emoji",
                                "name": "unicorn_face",
                                "unicode": "1f984"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U5STGTB3J",
        "type": "message",
        "ts": "1624441675.148100",
        "client_msg_id": "3bc5b5af-b16f-4d83-9f77-b95c3cc83433",
        "text": "Like the sentiment and think Rust and Swift are on a better path, mainly because they manage memory deterministically, but if people don't find garbage collection wasteful today in basic single-threaded CPU-bound scenarios, I doubt it'll stop anybody from bringing it to a distributed environment.\n\nClassic garbage collection makes a lot of sense for a simpler, centralized memory model, like a heap. I don't know how to adapt it to a massively parallel execution environment, but I'm sure it can be done somehow, likely involving an order-of-magnitude increase in (wasted) memory along the way, but it'll be much more convenient to use I'm sure.\n\nI don't know much about \"AI chips\", but if they are optimizing for parallel execution and work anything like modern GPUs they probably already manage memory quite differently from CPUs with explicit descriptors (a form of type system), buffer hierarchies, and thread grouping with localized access to buffers. That way memory gets bound to certain computations (shaders) and freed once these computations are finished. Well, it's a little more complex as these are subdivided into workgroups, thread groups, and subgroups, but you'll get the idea.\n\nIf you squint your eyes you might see some parallels to Rust's ownership model and why deterministic memory management is so attractive, even in languages that mainly target CPUs. The future is more value (move\/copy) and fewer reference semantics (\"objects\") with clear ownership to help avoid, or at least minimize, shared state, so we can reap the benefits of parallel processing. That's a promising approach to keep complexity in check, even though it might not quite feel like that yet when you're trying to configure a shader execution pipeline today.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "yGF",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Like the sentiment and think Rust and Swift are on a better path, mainly because they manage memory deterministically, but if people don't find garbage collection wasteful today in basic single-threaded CPU-bound scenarios, I doubt it'll stop anybody from bringing it to a distributed environment.\n\nClassic garbage collection makes a lot of sense for a simpler, centralized memory model, like a heap. I don't know how to adapt it to a massively parallel execution environment, but I'm sure it can be done somehow, likely involving an order-of-magnitude increase in (wasted) memory along the way, but it'll be much more convenient to use I'm sure.\n\nI don't know much about \"AI chips\", but if they are optimizing for parallel execution and work anything like modern GPUs they probably already manage memory quite differently from CPUs with explicit descriptors (a form of type system), buffer hierarchies, and thread grouping with localized access to buffers. That way memory gets bound to certain computations (shaders) and freed once these computations are finished. Well, it's a little more complex as these are subdivided into workgroups, thread groups, and subgroups, but you'll get the idea.\n\nIf you squint your eyes you might see some parallels to Rust's ownership model and why deterministic memory management is so attractive, even in languages that mainly target CPUs. The future is more value (move\/copy) and fewer reference semantics (\"objects\") with clear ownership to help avoid, or at least minimize, shared state, so we can reap the benefits of parallel processing. That's a promising approach to keep complexity in check, even though it might not quite feel like that yet when you're trying to configure a shader execution pipeline today."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBN9AFS0N",
        "type": "message",
        "ts": "1624451568.148300",
        "client_msg_id": "f319d932-0b7e-4660-91cb-fef086b68fc4",
        "text": "Pony's Garbage Collection sounds the closest I can think of: <https:\/\/tutorial.ponylang.io\/appendices\/garbage-collection.html>",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "7f0f1c0238ec",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-07-09\/395086754178_7f0f1c0238ec02befdab_72.jpg",
            "first_name": "Mariano",
            "real_name": "Mariano Guerra",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "mariano",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "z5nd",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Pony's Garbage Collection sounds the closest I can think of: "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/tutorial.ponylang.io\/appendices\/garbage-collection.html"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U019CPED6T1",
                    "U01T2EYBH0T",
                    "UHWC9PXBL"
                ],
                "count": 3
            }
        ]
    },
    {
        "user": "UBN9AFS0N",
        "type": "message",
        "ts": "1624451663.148500",
        "client_msg_id": "d3e0ff80-e2a8-41d7-b985-6b8c1d6bf7ef",
        "text": "There are some papers and talks about it",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "7f0f1c0238ec",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-07-09\/395086754178_7f0f1c0238ec02befdab_72.jpg",
            "first_name": "Mariano",
            "real_name": "Mariano Guerra",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "mariano",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "BV3Z",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "There are some papers and talks about it"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01PGQQEU2Z",
        "type": "message",
        "ts": "1624457980.148800",
        "client_msg_id": "75dd45a6-6cd9-4252-a039-52e8af4fec7a",
        "text": "Nim &amp; ATS do this as well.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0566a74707e4",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-02-28\/1803506076818_0566a74707e4edb4ea8a_72.jpg",
            "first_name": "",
            "real_name": "Deech",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "aditya.siram",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "GUj",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Nim & ATS do this as well."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UA14TGLTC",
        "type": "message",
        "ts": "1624463680.149000",
        "client_msg_id": "ea46e8d6-6ebc-46a5-a157-f01ae078123d",
        "text": "In my limited (yet significant) experience making a distributed system, the challenge wasn't so much garbage as ferrying relevant partial results between compute nodes.  So an ownership model may match bandwidth constraints better than potentially costly deferences.\n\nI guess it's an eager\/lazy distinction.  I mean GC is certainly a performance win if you never need to actually collect it.  Granularity matters.  A lot of programs operate in a sort of loop with a lot of objects allocated per frame or per request.  So then it makes sense to have an allocator that only tracks when references cross the boundary.  Squint and you can think of that as an eager generational collector.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gae6d55db9d1",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
            "first_name": "",
            "real_name": "William Taysom",
            "display_name": "wtaysom",
            "team": "T5TCAFTA9",
            "name": "wtaysom",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ebhBc",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "In my limited (yet significant) experience making a distributed system, the challenge wasn't so much garbage as ferrying relevant partial results between compute nodes.  So an ownership model may match bandwidth constraints better than potentially costly deferences.\n\nI guess it's an eager\/lazy distinction.  I mean GC is certainly a performance win if you never need to actually collect it.  Granularity matters.  A lot of programs operate in a sort of loop with a lot of objects allocated per frame or per request.  So then it makes sense to have an allocator that only tracks when references cross the boundary.  Squint and you can think of that as an eager generational collector."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "UCGAK10LS"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UT60XSVCN",
        "type": "message",
        "ts": "1624473008.149300",
        "client_msg_id": "a9f5a686-ef71-4b77-aa63-7ee06ccd8727",
        "text": "I'm not sure I understand the problem—you can have actor-local heaps and run gc independently while communicating via message-passing. Erlang does this",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gbc3e6041047",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/bc3e6041047849518d1b042f0a29d5af.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0020-72.png",
            "first_name": "",
            "real_name": "S.M Mukarram Nainar",
            "display_name": "S.M Mukarram Nainar",
            "team": "T5TCAFTA9",
            "name": "nainars",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "how",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'm not sure I understand the problem—you can have actor-local heaps and run gc independently while communicating via message-passing. Erlang does this"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UCGAK10LS",
        "type": "message",
        "ts": "1624496233.149800",
        "client_msg_id": "60d964d4-a6e9-4cc1-8843-867410203f67",
        "text": "<@UT60XSVCN> That's a solution if your programming model is the actor model. By \"transparent distribution\" I mean something more implicit: your programming language doesn't have a means to talk about \"nodes\" and \"messages\", so there are no clear boundaries for a garbage collector.\n\nNote that in Erlang, you essentially have automatic memory management (GC) _within_ an actor, and manual memory management _between_ actors. If you have an actor that is holding data that will later be queried by other actors, you need to know when it is safe to delete the data (or even the entire actor), i.e. you need to know when other actors are no longer holding \"references\" to it (of some kind).\n\nIt seems like Pony has an approach for inter-actor memory management. Thanks <@UBN9AFS0N> for the link. Though as I said before, the distribution model is still not quite as implicit as I had in mind :slightly_smiling_face:. I don't think you want to program a 1000-core AI chip with the actor model. ML models don't want to be written as actor systems, and general-purpose programs even less-so.\n\nI'm bringing up AI chips because some of them will be capable of running general-purpose programs. Think of them as the next step beyond GPGPU. <https:\/\/www.tenstorrent.com\/|Tenstorrent> is an example. From their FAQ:\n• \"Our computers are optimized for neural network inference and training. They can also execute other types of parallel computation.\"\n• \"Network communication hardware is present in each processor, and they talk with one another directly over (on-chip) networks, instead of through DRAM.\"\n• [Compared to GPUs] \"Our computers are easier to program, scale better, and are excellent at handling run-time sparsity and conditional computation.\"\nHopefully that answers your question <@U5STGTB3J>: the programming model is very different to that of GPUs.\n\nNote the irony that Tenstorrent's chips are an actor model at the hardware level, yet you're unlikely to want to program them using the actor model because of the sheer number of cores. What is our programming model for these machines? As stated in my original post, I think memory management needs to be explicit in the language (Rust-style), because you don't want all 1000+ cores to be running a distributed garbage collection scheme alongside the primary computation. It becomes less and less feasible the more cores you add. Tenstorrent is planning to have their chips plug directly together using high-bandwidth &amp; low-latency interconnects so that you can have 100,000 cores or more. Imagine running a garbage collector on that. Seems very much the wrong direction to go in.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "6402e9775ed7",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-04-13\/5095853045814_6402e9775ed73b75334f_72.jpg",
            "first_name": "",
            "real_name": "Nick Smith",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "nmsmith65",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1624437161.147900",
        "parent_user_id": "UCGAK10LS",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "mBLM",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UT60XSVCN"
                            },
                            {
                                "type": "text",
                                "text": " That's a solution if your programming model is the actor model. By \"transparent distribution\" I mean something more implicit: your programming language doesn't have a means to talk about \"nodes\" and \"messages\", so there are no clear boundaries for a garbage collector.\n\nNote that in Erlang, you essentially have automatic memory management (GC) "
                            },
                            {
                                "type": "text",
                                "text": "within",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " an actor, and manual memory management "
                            },
                            {
                                "type": "text",
                                "text": "between",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " actors. If you have an actor that is holding data that will later be queried by other actors, you need to know when it is safe to delete the data (or even the entire actor), i.e. you need to know when other actors are no longer holding \"references\" to it (of some kind).\n\nIt seems like Pony has an approach for inter-actor memory management. Thanks "
                            },
                            {
                                "type": "user",
                                "user_id": "UBN9AFS0N"
                            },
                            {
                                "type": "text",
                                "text": " for the link. Though as I said before, the distribution model is still not quite as implicit as I had in mind "
                            },
                            {
                                "type": "emoji",
                                "name": "slightly_smiling_face",
                                "unicode": "1f642"
                            },
                            {
                                "type": "text",
                                "text": ". I don't think you want to program a 1000-core AI chip with the actor model. ML models don't want to be written as actor systems, and general-purpose programs even less-so.\n\nI'm bringing up AI chips because some of them will be capable of running general-purpose programs. Think of them as the next step beyond GPGPU. "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.tenstorrent.com\/",
                                "text": "Tenstorrent"
                            },
                            {
                                "type": "text",
                                "text": " is an example. From their FAQ:\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "\"Our computers are optimized for neural network inference and training. They can also execute other types of parallel computation.\""
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "\"Network communication hardware is present in each processor, and they talk with one another directly over (on-chip) networks, instead of through DRAM.\""
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "[Compared to GPUs] \"Our computers are easier to program, scale better, and are excellent at handling run-time sparsity and conditional computation.\""
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nHopefully that answers your question "
                            },
                            {
                                "type": "user",
                                "user_id": "U5STGTB3J"
                            },
                            {
                                "type": "text",
                                "text": ": the programming model is very different to that of GPUs.\n\nNote the irony that Tenstorrent's chips are an actor model at the hardware level, yet you're unlikely to want to program them using the actor model because of the sheer number of cores. What is our programming model for these machines? As stated in my original post, I think memory management needs to be explicit in the language (Rust-style), because you don't want all 1000+ cores to be running a distributed garbage collection scheme alongside the primary computation. It becomes less and less feasible the more cores you add. Tenstorrent is planning to have their chips plug directly together using high-bandwidth & low-latency interconnects so that you can have 100,000 cores or more. Imagine running a garbage collector on that. Seems very much the wrong direction to go in."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]