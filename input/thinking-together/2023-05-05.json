[
    {
        "user": "U5STGTB3J",
        "type": "message",
        "ts": "1683284833.301809",
        "client_msg_id": "72B254A6-194D-4A27-8ADE-BB36563A707E",
        "text": "For those who’d like to dig deeper:\n\nThis is perhaps good as an introduction to attention, which is deeply interwoven with insight (There is a reason why the foundational transformer paper was called “_Attention_ is all you need”): <https:\/\/stefanlesser.substack.com\/p\/attention-and-insight|https:\/\/stefanlesser.substack.com\/p\/attention-and-insight>\n\nIn the post I mostly link to Vervaeke’s explanation in episode 9 of Awakening from the Meaning Crisis, which is about Insight: <https:\/\/www.youtube.com\/watch?v=jkWNBdBDyoE|https:\/\/www.youtube.com\/watch?v=jkWNBdBDyoE>\n\nThis is all still pretty basic stuff, but should already give you a good enough sense that you don’t need to run around scared that current LLMs might be sentient. But of course there’s more.\n\nEpisode 32 of that series connects more recent research about self-organizing criticality as a dynamic component to insight (It’s probably necessary, for sure helpful, to watch episode 31 first). That is not the only component, but is interwoven with a structural component of small-world networks in a framework called relevance realization. \n\nThat is all relatively recent theory, which means it’s a hypothesis that needs more validation, but if you follow the whole argument and it’s connections (which is a lot of work, so I don’t really expect anyone reading this actually spending the time and effort, but if you do, let me know!) it seems quite plausible and well-grounded in prior research. The details may not turn out to work exactly as described, but the overall connections made seem to already have good converging evidence.\n\nThe gist is: the relatively static LLMs we use at the moment have some of the structural capability but are missing the dynamic capability for insight. It could be argued that some of that happens during the learning phase (and I don’t understand the AI research side deeply enough to know if that is actually the case, but probably not), but as long as we have separate learning phases and not fully integrated live learning (and the tricks we use for fine tuning models and using custom embeddings don’t do that) we don’t have to be worried that what cognitive science understands insight to be to occur in current LLMs.\n\nIt is, however, not that far away, and both AI research and cognitive science are tightly interwoven to make progress on this. This is why I find what is outlined in this Relevance Realization paper quite fascinating: <http:\/\/www.ipsi.utoronto.ca\/sdis\/Relevance-Published.pdf|http:\/\/www.ipsi.utoronto.ca\/sdis\/Relevance-Published.pdf>\n\nThe good news is that as we get closer to creating consciousness, we also figure out how it works, and vice versa. The bad news is, by the time we fully understand what consciousness is and does, we will likely also have already created it.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679642239.661619",
        "parent_user_id": "UA14TGLTC",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "2uM6",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "For those who’d like to dig deeper:\n\nThis is perhaps good as an introduction to attention, which is deeply interwoven with insight (There is a reason why the foundational transformer paper was called “"
                            },
                            {
                                "type": "text",
                                "text": "Attention",
                                "style": {
                                    "bold": false,
                                    "italic": true,
                                    "strike": false
                                }
                            },
                            {
                                "type": "text",
                                "text": " is all you need”): "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/stefanlesser.substack.com\/p\/attention-and-insight",
                                "text": "https:\/\/stefanlesser.substack.com\/p\/attention-and-insight"
                            },
                            {
                                "type": "text",
                                "text": "\n\nIn the post I mostly link to Vervaeke’s explanation in episode 9 of Awakening from the Meaning Crisis, which is about Insight: "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.youtube.com\/watch?v=jkWNBdBDyoE",
                                "text": "https:\/\/www.youtube.com\/watch?v=jkWNBdBDyoE"
                            },
                            {
                                "type": "text",
                                "text": "\n\nThis is all still pretty basic stuff, but should already give you a good enough sense that you don’t need to run around scared that current LLMs might be sentient. But of course there’s more.\n\nEpisode 32 of that series connects more recent research about self-organizing criticality as a dynamic component to insight (It’s probably necessary, for sure helpful, to watch episode 31 first). That is not the only component, but is interwoven with a structural component of small-world networks in a framework called relevance realization. \n\nThat is all relatively recent theory, which means it’s a hypothesis that needs more validation, but if you follow the whole argument and it’s connections (which is a lot of work, so I don’t really expect anyone reading this actually spending the time and effort, but if you do, let me know!) it seems quite plausible and well-grounded in prior research. The details may not turn out to work exactly as described, but the overall connections made seem to already have good converging evidence.\n\nThe gist is: the relatively static LLMs we use at the moment have some of the structural capability but are missing the dynamic capability for insight. It could be argued that some of that happens during the learning phase (and I don’t understand the AI research side deeply enough to know if that is actually the case, but probably not), but as long as we have separate learning phases and not fully integrated live learning (and the tricks we use for fine tuning models and using custom embeddings don’t do that) we don’t have to be worried that what cognitive science understands insight to be to occur in current LLMs.\n\nIt is, however, not that far away, and both AI research and cognitive science are tightly interwoven to make progress on this. This is why I find what is outlined in this Relevance Realization paper quite fascinating: "
                            },
                            {
                                "type": "link",
                                "url": "http:\/\/www.ipsi.utoronto.ca\/sdis\/Relevance-Published.pdf",
                                "text": "http:\/\/www.ipsi.utoronto.ca\/sdis\/Relevance-Published.pdf"
                            },
                            {
                                "type": "text",
                                "text": "\n\nThe good news is that as we get closer to creating consciousness, we also figure out how it works, and vice versa. The bad news is, by the time we fully understand what consciousness is and does, we will likely also have already created it."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]