<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #thinking-together</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://secure.gravatar.com/avatar/5247a9c6cbb943683c9e2e2cef6eba79.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0022-72.png" style="float:left"/>
      <a href="../thinking-together/1721073254.172849.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Jason Morris</b>
<span style="margin:2em; color:#606060">2024-07-15 12:54</span><br/>
After reading a linkedin post by a friend of mine, today, in which he describes "protective randomness" as a virtue of human systems, the absence of which makes certain AI applications frightening, (<a href="https://www.linkedin.com/posts/colarusso_protective-randomness-why-we-fear-the-ai-activity-7218600315464413184-rpc2">https://www.linkedin.com/posts/colarusso_protective-randomne&hellip;</a>) I have come to the conclusion that this may be the best argument I have heard to object to the determinism in Rules as Code: that the systems we have for dealing with the output of the interpretation of statute not only anticipate but depend on variability of interpretation for their effective operation, and that they operate to do more than merely resolve disputes over legal interpretation, so the absence of those disputes is not an unmitigated benefit. Take for instance the conversation that happens between law makers and judges. A law is drafted, there are disputes over how to interpret it, in part fueled by examples of differing interpretations from administrative decision makers. Those differences in interpretation can help people know on what grounds they might object to a particular decision. Those objections go to judges, and judges might differ again, which has the same effect. And the differing interpretations of the judges in the context of things worth arguing about brings to legislators' attention places where the rule itself is both problematic from a fairness and/or interpretation standpoint, and where that problem causes significant issues in the real world, equipping them with information about where to focus when considering amendments. All of those benefits accrue even if no decision is ever made to resolve the dispute. If you replace variable administrative decision makers with a deterministic system based on a best shared interpretation, that review system, which not only accounts for but depends on variable outputs to function, stops working as effectively. That is not to say that what you lose is not worth what you gain. But it is a coherent argument about what is lost, and how, from consistent automated application of laws, and it is prescriptive about where and how rules as code should best be applied to avoid those losses. Which now makes me wonder whether armed with this idea, I would have a more generous read of the Laurence Driver paper.<br/><br/>For context, the linked post is aimed more at attempting to understand some forms of anxiety around AI applications, and particularly in the context of copyright violation, which Colarusso considers complaining about the wrong thing. But the idea metastasized in my head, and now I might have to give Driver the benefit of the doubt.
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
