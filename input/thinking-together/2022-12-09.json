[
    {
        "user": "UA14TGLTC",
        "type": "message",
        "ts": "1670577496.554869",
        "client_msg_id": "b3ccf8af-a44c-4c8e-a5de-750f6b764840",
        "text": "Correctness is certainly a problem people are working on.  This came across my radar, \"We also find that [our technique] cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.\"\n\nAnother technique that comes to mind is having the LLM carry on a conversation with an oracle.  For instance, if it's trying to write code, it will try running it before coming back to the human user.",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gae6d55db9d1",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
            "first_name": "",
            "real_name": "William Taysom",
            "display_name": "wtaysom",
            "team": "T5TCAFTA9",
            "name": "wtaysom",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1670152748.796319",
        "parent_user_id": "UJFN50C00",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "2eVkp",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Correctness is certainly a problem people are working on.  This came across my radar, \"We also find that [our technique] cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.\"\n\nAnother technique that comes to mind is having the LLM carry on a conversation with an oracle.  For instance, if it's trying to write code, it will try running it before coming back to the human user."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]