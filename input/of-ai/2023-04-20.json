[
    {
        "user": "U01L6HZEHFX",
        "type": "message",
        "ts": "1681986431.736239",
        "edited": {
            "user": "U01L6HZEHFX",
            "ts": "1681986471.000000"
        },
        "client_msg_id": "e9ddb0aa-255d-484b-b2e0-5407447d39fc",
        "text": "I don't know if I want to laugh of cry :joy:. This is done just by duct-taping a bunch of APIs and GPT-4 prompts together. Maybe the future of programming will be more English than code? <https:\/\/chameleon-llm.github.io\/>. Paper (<https:\/\/arxiv.org\/abs\/2304.09842>) and Code (<https:\/\/github.com\/lupantech\/chameleon-llm>)",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "3a9710d9a208",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-01-29\/1686621807717_3a9710d9a208814fffb1_72.jpg",
            "first_name": "Thanh",
            "real_name": "Thanh Dinh",
            "display_name": "Thanh Dinh",
            "team": "T5TCAFTA9",
            "name": "thanhdk",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "attachments": [
            {
                "from_url": "https:\/\/arxiv.org\/abs\/2304.09842",
                "service_icon": "https:\/\/static.arxiv.org\/static\/browse\/0.3.4\/images\/icons\/apple-touch-icon.png",
                "thumb_url": "https:\/\/static.arxiv.org\/static\/browse\/0.3.4\/images\/arxiv-logo-fb.png",
                "thumb_width": 1200,
                "thumb_height": 700,
                "id": 1,
                "original_url": "https:\/\/arxiv.org\/abs\/2304.09842",
                "fallback": "arXiv.org: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
                "text": "Large language models (LLMs) have achieved remarkable progress in various\nnatural language processing tasks with emergent abilities. However, they face\ninherent limitations, such as an inability to access up-to-date information,\nutilize external tools, or perform precise mathematical reasoning. In this\npaper, we introduce Chameleon, a plug-and-play compositional reasoning\nframework that augments LLMs to help address these challenges. Chameleon\nsynthesizes programs to compose various tools, including LLM models,\noff-the-shelf vision models, web search engines, Python functions, and\nrule-based modules tailored to user interests. Built on top of an LLM as a\nnatural language planner, Chameleon infers the appropriate sequence of tools to\ncompose and execute in order to generate a final response. We showcase the\nadaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP.\nNotably, Chameleon with GPT-4 achieves an 86.54% accuracy on ScienceQA,\nsignificantly improving upon the best published few-shot model by 11.37%; using\nGPT-4 as the underlying LLM, Chameleon achieves a 17.8% increase over the\nstate-of-the-art model, leading to a 98.78% overall accuracy on TabMWP. Further\nstudies suggest that using GPT-4 as a planner exhibits more consistent and\nrational tool selection and is able to infer potential constraints given the\ninstructions, compared to other LLMs like ChatGPT.",
                "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
                "title_link": "https:\/\/arxiv.org\/abs\/2304.09842",
                "service_name": "arXiv.org"
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "pcG27",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I don't know if I want to laugh of cry "
                            },
                            {
                                "type": "emoji",
                                "name": "joy",
                                "unicode": "1f602"
                            },
                            {
                                "type": "text",
                                "text": ". This is done just by duct-taping a bunch of APIs and GPT-4 prompts together. Maybe the future of programming will be more English than code? "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/chameleon-llm.github.io\/"
                            },
                            {
                                "type": "text",
                                "text": ". Paper ("
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/arxiv.org\/abs\/2304.09842"
                            },
                            {
                                "type": "text",
                                "text": ") and Code ("
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/github.com\/lupantech\/chameleon-llm"
                            },
                            {
                                "type": "text",
                                "text": ")"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "thinking_face",
                "users": [
                    "UA14TGLTC"
                ],
                "count": 1
            }
        ]
    }
]