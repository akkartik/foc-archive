<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #thinking-together</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <img src="https://avatars.slack-edge.com/2021-09-28/2536072635510_da03e6d17065db49a1d3_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Diego Moya</b>
<span style="margin:2em; color:#606060">2021-10-04 10:55</span><br/>
Artificial Intelligence is known to be a generic umbrella term for a wide variety of techniques. The current AI renaissance based on Machine Learning is all about advanced statistical techniques, and they're getting awesome never-seen-before results in all kinds of artistic media or tasks requiring observation and adequate reactions. Yet it often feels like these techniques don't really understand the problem they're solving, they merely act by imitation of what they were trained on.<br/><br/>Classic AI, the one based on logic inferences, is strong in that task of understanding the situation and giving precise answers. Yet it lacks intuition, often resorts to brute force, and has not been seen to be able to generate anything resembling creativity (or not on the levels of the Deep Learning). I have often wondered if there would be a way to combine the strengths of both, but I know of no research that has attempted to do that.<br/><br/>Do you know of any techniques that combines ML with deductive reasoning, using the first to "learn" about a problem domain and the second to "clean up" inconsistencies and errors in the solutions created "by gut feel" with the former?
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633371136.277000"></a>
      <img src="https://avatars.slack-edge.com/2020-09-09/1376906509376_a07cdcb6d037bf7b6a5e_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633371136.277000" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Denny Vrandečić</b>
<span style="margin:2em; color:#606060">2021-10-04 11:12</span><br/>
Some terms I have seen for that is "neuro-symbolic integration" or "hybrid AI", eg. <a href="https://arxiv.org/abs/2012.05876">https://arxiv.org/abs/2012.05876</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633393666.278600"></a>
      <img src="https://avatars.slack-edge.com/2023-04-13/5095853045814_6402e9775ed73b75334f_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633393666.278600" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Nick Smith</b>
<span style="margin:2em; color:#606060">2021-10-04 17:27</span><br/>
<a href="https://www.relational.ai/">RelationalAI</a> seems to be exploring this intersection. Perhaps <span style="background-color:#ccf">@Molham Aref</span> has something to say :innocent:.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633403135.278800"></a>
      <img src="https://avatars.slack-edge.com/2021-04-06/1940170661956_4c418a286646efdf7bb4_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633403135.278800" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Molham Aref</b>
<span style="margin:2em; color:#606060">2021-10-04 20:05</span><br/>
indeed <span style="background-color:#ccf">@Nick Smith</span>, the work we are doing at RelationalAI is at the intersection of statistical and logical/relational modeling.  There are so many ways to combine these approaches.  Check out this overview talk by <a href="https://www.youtube.com/watch?v=_cQITY0SPiw">Henry Kautz</a> at AAAI 2020 which influenced a talk by <a href="https://ibm.ent.box.com/s/3kgxhuy8h9x98nnck26a0498q9rjkwhp">Alex Gray</a> at IBM’s Advances in Neuro-Symbolic AI <a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=10510">Seminar Series</a>.  I hope that helps.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633520851.281600"></a>
      <img src="https://avatars.slack-edge.com/2021-07-07/2254853369060_d6900487d9109f495c79_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633520851.281600" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Alexander Chichigin</b>
<span style="margin:2em; color:#606060">2021-10-06 04:47</span><br/>
If you're interested in modern AI research (and not only merging ML and logic) you should look at "Artificial General Intelligence" -- this is contemporary term for "actual AI", they have an annual conference, journals and many other things. As well as several research avenues and approaches, some of them combining others (presumably) subsuming both ML and logical reasoning.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633554344.282400"></a>
      <img src="https://avatars.slack-edge.com/2021-06-21/2186437719222_d0f48a5cbd367fc3a50b_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633554344.282400" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>dnmfarrell</b>
<span style="margin:2em; color:#606060">2021-10-06 14:05</span><br/>
If you're interested in understanding the limitations of mainstream AI approaches you might like these books: The Myth of Artificial Intelligence by Larson and Rebooting AI by Marcus and Davis. And this recent article: <a href="https://spectrum.ieee.org/deep-learning-computational-cost">https://spectrum.ieee.org/deep-learning-computational-cost</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633554546.282800"></a>
      <img src="https://avatars.slack-edge.com/2021-06-21/2186437719222_d0f48a5cbd367fc3a50b_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633554546.282800" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>dnmfarrell</b>
<span style="margin:2em; color:#606060">2021-10-06 14:09</span><br/>
It sounds like you're talking about logical abduction (instead of deduction/induction) which Larson discusses. Forming hypotheses based on experience that may or not be true but are better than random guesses and cheaper to calculate. Inference
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633577556.283200"></a>
      <img src="https://secure.gravatar.com/avatar/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633577556.283200" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>William Taysom</b>
<span style="margin:2em; color:#606060">2021-10-06 20:32</span><br/>
It does seem that these statistical techniques somehow end up modeling what people would call gut reactions: the first thing you think of when you haven't yet thought things through.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633674506.286900"></a>
      <img src="https://avatars.slack-edge.com/2021-07-07/2254853369060_d6900487d9109f495c79_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633674506.286900" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Alexander Chichigin</b>
<span style="margin:2em; color:#606060">2021-10-07 23:28</span><br/>
<span style="background-color:#ccf">@William Taysom</span> I'm pretty sure that's exactly because both "gut feeling" and ML are based on pattern recognition and matching. :slightly_smiling_face: Patterns are the first thing brain attains to. Only when it fails we reach for <em>models</em>.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633691923.287100"></a>
      <img src="https://avatars.slack-edge.com/2021-06-21/2186437719222_d0f48a5cbd367fc3a50b_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633691923.287100" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>dnmfarrell</b>
<span style="margin:2em; color:#606060">2021-10-08 04:18</span><br/>
Larson claims the limitation with AI is it can only recognize patterns it has seen. Take self driving cars for example. If you're driving behind a pickup truck on the highway and a birthday balloon falls off the back of it, you probably realize it is safe to drive over it. You've never seen this happen before but you know that balloons are filled with air, and its movement suggests it too. What about an AI? Well if it hasn't been trained on this scenario, perhaps it slams on the brakes and causes a collision.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633693663.287700"></a>
      <img src="https://secure.gravatar.com/avatar/3ae6d55db9d15b79bc683a8031fc2588.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633693663.287700" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>William Taysom</b>
<span style="margin:2em; color:#606060">2021-10-08 04:47</span><br/>
I can tell you that Teslas freak out over a lot less than a balloon.  It's a lot less of an "autopilot" than mixed initiative automatically adjusting autonomous system.  I haven't used those words in a long time.  <a href="https://www.semanticscholar.org/paper/Dimensions-of-Adjustable-Autonomy-and-Interaction-Bradshaw-Feltovich/e4bfe3b40d36f0b8c79c4c98319d4bf51569d080">https://www.semanticscholar.org/paper/Dimensions-of-Adjustab&hellip;</a>
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1633945296.319900"></a>
      <img src="https://avatars.slack-edge.com/2021-07-07/2254853369060_d6900487d9109f495c79_72.jpg" style="float:left"/>
      <a href="../thinking-together/1633370115.276200.html#1633945296.319900" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Alexander Chichigin</b>
<span style="margin:2em; color:#606060">2021-10-11 02:41</span><br/>
<span style="background-color:#ccf">@dnmfarrell</span> technically the purpose of <em>predictive</em> Machine Learning models is to <em>generalize</em> (somewhat) beyond what they saw in the training data. And indeed they are evaluated on data they never saw in training. But in reality this generalization goes only this far, and not really applicable to kinda "open world" scenarios like driving.
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
