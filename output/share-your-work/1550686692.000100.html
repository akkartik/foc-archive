<html>
<head><meta charset="UTF-8"></head><h2>Archives, <a href="https://futureofcoding.org/community">Future of Coding Community</a>, #share-your-work</h2>
  <table>
  <tr>
    <td style="width:72px; vertical-align:top; padding-bottom:1em">
      <a href="../share-your-work/1550686692.000100.html" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Brian Hempel</b>
<span style="margin:2em; color:#606060">2019-02-20 10:18</span><br/>
When working on visionary systems or systems designed for experts, the academic publishing review process can sometimes be brutal. Reviewers sometimes expect user studies, but complex systems, particularly systems built for experts, cannot be compared head-to-head with prior work in 1-2hr user study sessions—fluency with a <em>tool</em> could require 6 months practice or more! How then do you show that the system you’ve built is actually an improvement, if you can’t perform traditional human testing on it? In 2007, Dan Olsen addressed this problem, identifying the common reviewer mistakes (“Evaluation Errors”), outlining a framework for justifying the contribution of a complex system (“STU Context”), and offering possible claims a paper could make in leu of human usability tests. To help me keep track of these ideas, I pulled them into a one-page summary. I hope others find this summary helpful as well.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1550687026.000400"></a>
      <a href="../share-your-work/1550686692.000100.html#1550687026.000400" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Brian Hempel</b>
<span style="margin:2em; color:#606060">2019-02-20 10:23</span><br/>
(PDF)
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1550688816.000800"></a>
      <img src="https://secure.gravatar.com/avatar/ce240a8e5a8fdc65e86bbb869975ccfe.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png" style="float:left"/>
      <a href="../share-your-work/1550686692.000100.html#1550688816.000800" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Will Crichton</b>
<span style="margin:2em; color:#606060">2019-02-20 10:53</span><br/>
Nice writeup! I’m not sure if this was in the original paper, but it seems like another important quality to assess is stability/maintainability/longevity. Whether the qualities of a system minimize friction/errors when transitioning between developers, teams, or organizations. Could the quality of the Linux kernel be evaluated for its longevity, etc.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1550692001.001200"></a>
      <img src="https://avatars.slack-edge.com/2018-08-03/410049852848_d79ba47b2952e74a99d9_72.jpg" style="float:left"/>
      <a href="../share-your-work/1550686692.000100.html#1550692001.001200" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Brian Hempel</b>
<span style="margin:2em; color:#606060">2019-02-20 11:46</span><br/>
I’m sure Olsen’s list isn’t exhaustive, so I’m curious to hear what other criteria people might propose. Maintainability is important, but I’m not sure how you’d measure it—particularly for an academic paper about a system you’ve just created!
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1550692533.001400"></a>
      <img src="https://secure.gravatar.com/avatar/ce240a8e5a8fdc65e86bbb869975ccfe.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png" style="float:left"/>
      <a href="../share-your-work/1550686692.000100.html#1550692533.001400" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Will Crichton</b>
<span style="margin:2em; color:#606060">2019-02-20 11:55</span><br/>
Indeed. Perhaps if you could identify the core tasks in maintenance/refactoring, e.g. finding the component of a system responsible for a high-level task, or splitting a system between two teams, etc., then you could do targeted experiments to test whether those tasks are feasible for a moderately large codebase written in your system.
    </td>
  </tr>
  <tr>
    <td style="vertical-align:top; padding-bottom:1em">
      <a name="1550695799.001600"></a>
      <img src="https://avatars.slack-edge.com/2018-09-11/433781465829_7c31dc735c6c1257fe1f_72.jpg" style="float:left"/>
      <a href="../share-your-work/1550686692.000100.html#1550695799.001600" style="color:#aaa">#</a>
    </td>
    <td style="vertical-align:top; padding-bottom:1em; padding-left:1em">
<b>Steve Krouse</b>
<span style="margin:2em; color:#606060">2019-02-20 12:49</span><br/>
I love this! Thanks for sharing. Reminds me of the cognitive dimensions of notation framework
    </td>
  </tr>
  </table>
<hr>
<a href="https://akkartik.name/foc-archive.zip">download this site</a> (~25MB)<br/>
<a href="https://github.com/akkartik/foc-archive">Git repo</a>
</html>
